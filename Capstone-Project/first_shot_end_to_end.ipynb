{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T10:23:18.971585Z",
     "start_time": "2021-05-31T10:23:18.969299Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T10:23:18.976676Z",
     "start_time": "2021-05-31T10:23:18.973788Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.18.5 2.5.0 2.5.0\n"
     ]
    }
   ],
   "source": [
    "print(np.__version__, tf.__version__, keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T10:23:18.983664Z",
     "start_time": "2021-05-31T10:23:18.979282Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_tweet(tweet: str) -> str:\n",
    "    # take out URLs\n",
    "    tweet = re.sub(\"http.*(\\s|$)\", ' ', tweet)\n",
    "    # take out mentions\n",
    "    tweet = re.sub(\"@[^\\s]+\", ' ', tweet)\n",
    "    # take out hashtags\n",
    "    tweet = re.sub(\"#[^\\s]+\", ' ', tweet)\n",
    "    # take out all characters outside of those we enumerate\n",
    "    tweet = re.sub(\"[^\\da-zA-Z√°√©√≠√≥√∫√º√±√Å√â√ç√ë√ì√ö√ú¬ø?¬°!.,;#:<>()'‚Äú‚Äù\\\"\\s]\", ' ', tweet)\n",
    "    # lowercase everything\n",
    "    tweet = tweet.lower()\n",
    "    # reset spaces\n",
    "    tweet = re.sub(\"\\s+\", ' ', tweet)\n",
    "    tweet = re.sub(\"^\\s+\", '', tweet)\n",
    "    tweet = re.sub(\"\\s$\", '', tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T10:23:19.004393Z",
     "start_time": "2021-05-31T10:23:18.985755Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idk if it 's me getting old or what it is but I can n't stay up late anymore by like 9 or 10 i 'm already asleep lmao .\n",
      "idk if it 's me getting old or what it is but i can n't stay up late anymore by like 9 or 10 i 'm already asleep lmao .\n",
      "@HaiVikk__ @TedOfficialPage i got insomina nigga :( lmao\n",
      "i got insomina nigga :( lmao\n",
      "Aqu√≠ andamos @ElvisaYM y yo preparando el show de ma√±ana . üòÉ http://t.co/fFXRPpdzp0\n",
      "aqu√≠ andamos y yo preparando el show de ma√±ana .\n",
      "@teresa_perez7 @zapi_z @luciasdurand mira pendejita mas vale que termines yendo para Maya con Vale .. Yo me voy jueves\n",
      "mira pendejita mas vale que termines yendo para maya con vale .. yo me voy jueves\n",
      "Lista para recibir mi premio #mujerImagen2015 Gracias Dios por tantas bendiciones .. . Y por tener a ‚Ä¶ https://t.co/vklRriVvDZ\n",
      "lista para recibir mi premio gracias dios por tantas bendiciones .. . y por tener a\n",
      "\" @amberenee288 : @AnibalTornes wow , thaaaaaaaaaaanks . \" dont sweat it , try to sleep now if the excitement lets you of course .\n",
      "\" : wow , thaaaaaaaaaaanks . \" dont sweat it , try to sleep now if the excitement lets you of course .\n",
      "@chelita1123 @princessstacy_ Lmao I Would Pero You Know I Do n't want him to think I 'm weird so yes Ay Despues (;\n",
      "lmao i would pero you know i do n't want him to think i 'm weird so yes ay despues (;\n",
      "@AlanTorres8484 @27groberto is hooters better than pluckers on the food ?\n",
      "is hooters better than pluckers on the food ?\n",
      "@arelyyy_090812 @Jona_Boy13 üò±\n",
      "\n",
      "Me and you playing fifa , no se piensalo ? Lol xD\n",
      "me and you playing fifa , no se piensalo ? lol xd\n",
      "Been wanting to go out and do something for a while now but I never got the chance\n",
      "been wanting to go out and do something for a while now but i never got the chance\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# spotchecking loop to check that clean_tweet() works\n",
    "with open('lid_train_lines.txt', 'r') as input_file:\n",
    "    for line in input_file:\n",
    "        if random.random() < 0.0005:\n",
    "            line = line.strip()\n",
    "            print(line)\n",
    "            print(clean_tweet(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T10:23:19.319768Z",
     "start_time": "2021-05-31T10:23:19.006325Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_lines = []\n",
    "\n",
    "with open('lid_train_lines.txt', 'r') as input_file:\n",
    "    for line in input_file:\n",
    "        clean_lines.append(clean_tweet(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T10:23:19.326345Z",
     "start_time": "2021-05-31T10:23:19.322808Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21030"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T10:23:19.331182Z",
     "start_time": "2021-05-31T10:23:19.328947Z"
    }
   },
   "outputs": [],
   "source": [
    "random.seed(a=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T10:23:19.335266Z",
     "start_time": "2021-05-31T10:23:19.332975Z"
    }
   },
   "outputs": [],
   "source": [
    "chosen_tweets = random.choices(clean_lines, k=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T10:23:19.342062Z",
     "start_time": "2021-05-31T10:23:19.336994Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ma√±ana viernes wilfrido vargas ..',\n",
       " 'u love me',\n",
       " 'so original .. .',\n",
       " 'whats up with nature',\n",
       " 'aguanta tu ritmo .',\n",
       " \"i do n't like ( xxx ) people , they always think they 're right and do n't listen to anything else . ( testarudo a obstinado a tercho a )\",\n",
       " 'ni pinches moscas pescas tu mondaooo',\n",
       " 'new week , be kind and sweet ! vestido sahara',\n",
       " 'no puedo bregar con la gente stalker',\n",
       " 'a cantarle las ma√±anitas ala virgencita',\n",
       " 'el vlog de hoy aunque sea tarde , peque√±as cenas agradables : el d√≠a m√°s feliz de mi vida',\n",
       " 'all bad',\n",
       " 'no es sin√≥nimo pero significa lo mismo , i get out of work at 6 pm , creo q esto es lo q quer√≠as decir :)',\n",
       " 'blusa violeta y sus diferentes estampados ! escoge tu color favorito made in ecuador',\n",
       " '¬°¬° nuevo v√≠deo !! en esta ocasi√≥n , un famoso presentador de tv me hace una dur√≠sima entrevista .. . ¬° se .. .',\n",
       " 'get the hang of it : acostumbrarse a esto poder dominarlo person 1 : this new job is so stressful . person 2 : you ll get the hang of it .',\n",
       " 'clase de text',\n",
       " 'voy a empezar a pedirle trabajos a la gente como me los piden a mi',\n",
       " 'en estos momentos prefiero hacer mi ensayo de psicolog√≠a que trabajar',\n",
       " 'online class de sexual harassment .. . arianna is so mean',\n",
       " '\" y nunca me entretengo aber si me aman , les doy mi corazon por una semana , y luego dejo k se alejen de mi si les da la gana \"',\n",
       " \"ruth likes my dad 's new truck\",\n",
       " '‚Äú : si el camino es dif√≠cil .. . normalmente es porque vas en la direcci√≥n correcta . ‚Äù very true !! !',\n",
       " 'been waiting haha',\n",
       " 'lol jk but gracias corazon',\n",
       " '\" : my best friend',\n",
       " 'gracias flaca y el vestido es de la marca .',\n",
       " 'haber saludao !',\n",
       " 'u love me',\n",
       " 'unless they cut my hours again and tell me bout to go in today',\n",
       " 'es fun ver lo que dicen y eso jaja .',\n",
       " 'this bop put bacon on my food omfg foh',\n",
       " '',\n",
       " 'yo no pregunte pq a la defensiva ?',\n",
       " 'comenz√≥ la recta final a 1 mes y 1 d√≠a !! ! omg : o',\n",
       " 'la respuesta de es :',\n",
       " 'tu sabes que no',\n",
       " 'a escucharlo y disfrutarlo ! ( si te gusta claro )',\n",
       " 'te relacionas ? jajaja',\n",
       " '‚Äú : i miss my favorite stylist ! ‚Äù miss u to chica',\n",
       " 'ay y yo , pero tengo el estomago vacio y me iria over',\n",
       " 'si quieren un poco de inspiracion busquen el poema de steve jobs \" here \\'s to the crazy ones \"',\n",
       " 'fuhk valley is gunna close already',\n",
       " 'so si te van aser ?',\n",
       " 'a la verdad que el verano despues de freshmen year yo estaba obesa',\n",
       " \"plain and simple . simple y llanamente . i do n t need a new car , i 'm not interested , plain and simple\",\n",
       " 'as√≠ o m√°s energ√≠a ? la mayor cantidad de m√∫sica',\n",
       " 'that was so emberrasing',\n",
       " 'lol your all bad',\n",
       " 'tfti !! i want chokomil lmao',\n",
       " 'mi perrita abda de latosa y no me deja dormir .',\n",
       " 'me estan comprando una mac',\n",
       " 'a tus ni√±as les gusts el mundo del modelaje ? registralas en imagen modeling by la',\n",
       " \"yessss !! it does :d nd it 's gonna help us muajajaja . ;)\",\n",
       " 'gastar toel dinero que hiciste ?',\n",
       " 'buen provecho !',\n",
       " 'todos ayer jajaja',\n",
       " 'me metieron con un guineo en la cara ! jajajajjaj',\n",
       " 'quisiera volver a ser peque√±o , no pagar renta , ni usar desodrante .',\n",
       " '‚Äú : que guapa esperemos pronto verte por mexico saludos de todo el club de hp ‚Äù gracias chica . saludos a todos',\n",
       " 'go with the flow : varias cosas una sigue a la mayor√≠a p1 : are we going to plan the trip ? p2 : let s just go with the flow and see what happen',\n",
       " \"jajajajja ayy precious have you tried the chocolate filled gansitos ?? they 're sooooo good !!!\",\n",
       " 'la he escuchado mucho , pero no la usaba :)',\n",
       " 'can you ? lol',\n",
       " 'tengo el brazo como el muslo de beyonc√© , pero totally worth it . buenos d√≠as .',\n",
       " 'hate being so detailed when describing a person because i hit their qualities spot on when they describe me they make me sound so blah .',\n",
       " 'really what is it about ? i wanted too go see it lmao',\n",
       " 'all my tios are coming to my house .... like always',\n",
       " 'jajajajaja mira ponte a estudiar para el depa de ma√±ana y shh',\n",
       " 'life long dream ? ir a un concierto de coldplay',\n",
       " 'beffas beers pool',\n",
       " 'imagen modeling by la gatita reg√≠strate en',\n",
       " 'la primera es la correcta , no me expliqu√© bien :( can you tell me who she is , can you tell me where it is , do you know who he is :)',\n",
       " 'no',\n",
       " 'hay alguien que o no hablo en todo el dia o tiene la mente a millon',\n",
       " 'down like a real friends supposed to im trying to show you the life of somebody like you should be living',\n",
       " 'gracias',\n",
       " \"oh no ! let 's figure this out , edgar . could you please follow us and send us a dm for further assistance ? thanks ! mh\",\n",
       " 'gracias mi ponchin !',\n",
       " 'make them your strength',\n",
       " 'con cari√±o , una canci√≥n , . con√©ctate a la fiesta',\n",
       " '‚Äú : gracias via x mi para √±a2014 ‚Äù',\n",
       " 'jajajaja yo',\n",
       " 'my brothers are honestly amazing !!! i loved every single one of the presents they gave me !',\n",
       " 'no lmfao i was just not sleepy jajaja .',\n",
       " 'lo mejor hasta ahorita son y mi bella',\n",
       " '\" : alcoholic ? no , i prefer the term drinking enthusiast . \" jajajajaja accurate ?',\n",
       " 'me maltrato con un guineo .',\n",
       " 'nuestras faldas lucianas en sus diferentes colores para cada personalidad cu√°l prefieres ?',\n",
       " 'obsesionada con este personaje',\n",
       " '\" que mucha mierda has comido con la monografia esa .. \" pues hasla tu que tal ?',\n",
       " 'belvita !',\n",
       " 'no dejes que el lunes te ponga patas arriba ! empieza la semana con √°nimo . mi',\n",
       " 'para una noticiero estelar es necesario un outfit ! lucete como nuestra querida',\n",
       " 'los amo . ; un besote a mi por confiar en mi y a ustedes tambien .',\n",
       " \"lol okay it 's iaavalos\",\n",
       " \"omg this sucks !! i hate being sick can n't even breath :(\",\n",
       " '‚Äú : to err is human , to forgive , devine . ‚Äù',\n",
       " 'exacto',\n",
       " 'your not going to find a girl who loves you more than me']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chosen_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T10:23:19.350380Z",
     "start_time": "2021-05-31T10:23:19.343776Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ma√±ana viernes wilfrido vargas ..\n",
      "u love me\n",
      "so original .. .\n",
      "whats up with nature\n",
      "aguanta tu ritmo .\n",
      "i do n't like ( xxx ) people , they always think they 're right and do n't listen to anything else . ( testarudo a obstinado a tercho a )\n",
      "ni pinches moscas pescas tu mondaooo\n",
      "new week , be kind and sweet ! vestido sahara\n",
      "no puedo bregar con la gente stalker\n",
      "a cantarle las ma√±anitas ala virgencita\n",
      "el vlog de hoy aunque sea tarde , peque√±as cenas agradables : el d√≠a m√°s feliz de mi vida\n",
      "all bad\n",
      "no es sin√≥nimo pero significa lo mismo , i get out of work at 6 pm , creo q esto es lo q quer√≠as decir :)\n",
      "blusa violeta y sus diferentes estampados ! escoge tu color favorito made in ecuador\n",
      "¬°¬° nuevo v√≠deo !! en esta ocasi√≥n , un famoso presentador de tv me hace una dur√≠sima entrevista .. . ¬° se .. .\n",
      "get the hang of it : acostumbrarse a esto poder dominarlo person 1 : this new job is so stressful . person 2 : you ll get the hang of it .\n",
      "clase de text\n",
      "voy a empezar a pedirle trabajos a la gente como me los piden a mi\n",
      "en estos momentos prefiero hacer mi ensayo de psicolog√≠a que trabajar\n",
      "online class de sexual harassment .. . arianna is so mean\n",
      "\" y nunca me entretengo aber si me aman , les doy mi corazon por una semana , y luego dejo k se alejen de mi si les da la gana \"\n",
      "ruth likes my dad 's new truck\n",
      "‚Äú : si el camino es dif√≠cil .. . normalmente es porque vas en la direcci√≥n correcta . ‚Äù very true !! !\n",
      "been waiting haha\n",
      "lol jk but gracias corazon\n",
      "\" : my best friend\n",
      "gracias flaca y el vestido es de la marca .\n",
      "haber saludao !\n",
      "u love me\n",
      "unless they cut my hours again and tell me bout to go in today\n",
      "es fun ver lo que dicen y eso jaja .\n",
      "this bop put bacon on my food omfg foh\n",
      "\n",
      "yo no pregunte pq a la defensiva ?\n",
      "comenz√≥ la recta final a 1 mes y 1 d√≠a !! ! omg : o\n",
      "la respuesta de es :\n",
      "tu sabes que no\n",
      "a escucharlo y disfrutarlo ! ( si te gusta claro )\n",
      "te relacionas ? jajaja\n",
      "‚Äú : i miss my favorite stylist ! ‚Äù miss u to chica\n",
      "ay y yo , pero tengo el estomago vacio y me iria over\n",
      "si quieren un poco de inspiracion busquen el poema de steve jobs \" here 's to the crazy ones \"\n",
      "fuhk valley is gunna close already\n",
      "so si te van aser ?\n",
      "a la verdad que el verano despues de freshmen year yo estaba obesa\n",
      "plain and simple . simple y llanamente . i do n t need a new car , i 'm not interested , plain and simple\n",
      "as√≠ o m√°s energ√≠a ? la mayor cantidad de m√∫sica\n",
      "that was so emberrasing\n",
      "lol your all bad\n",
      "tfti !! i want chokomil lmao\n",
      "mi perrita abda de latosa y no me deja dormir .\n",
      "me estan comprando una mac\n",
      "a tus ni√±as les gusts el mundo del modelaje ? registralas en imagen modeling by la\n",
      "yessss !! it does :d nd it 's gonna help us muajajaja . ;)\n",
      "gastar toel dinero que hiciste ?\n",
      "buen provecho !\n",
      "todos ayer jajaja\n",
      "me metieron con un guineo en la cara ! jajajajjaj\n",
      "quisiera volver a ser peque√±o , no pagar renta , ni usar desodrante .\n",
      "‚Äú : que guapa esperemos pronto verte por mexico saludos de todo el club de hp ‚Äù gracias chica . saludos a todos\n",
      "go with the flow : varias cosas una sigue a la mayor√≠a p1 : are we going to plan the trip ? p2 : let s just go with the flow and see what happen\n",
      "jajajajja ayy precious have you tried the chocolate filled gansitos ?? they 're sooooo good !!!\n",
      "la he escuchado mucho , pero no la usaba :)\n",
      "can you ? lol\n",
      "tengo el brazo como el muslo de beyonc√© , pero totally worth it . buenos d√≠as .\n",
      "hate being so detailed when describing a person because i hit their qualities spot on when they describe me they make me sound so blah .\n",
      "really what is it about ? i wanted too go see it lmao\n",
      "all my tios are coming to my house .... like always\n",
      "jajajajaja mira ponte a estudiar para el depa de ma√±ana y shh\n",
      "life long dream ? ir a un concierto de coldplay\n",
      "beffas beers pool\n",
      "imagen modeling by la gatita reg√≠strate en\n",
      "la primera es la correcta , no me expliqu√© bien :( can you tell me who she is , can you tell me where it is , do you know who he is :)\n",
      "no\n",
      "hay alguien que o no hablo en todo el dia o tiene la mente a millon\n",
      "down like a real friends supposed to im trying to show you the life of somebody like you should be living\n",
      "gracias\n",
      "oh no ! let 's figure this out , edgar . could you please follow us and send us a dm for further assistance ? thanks ! mh\n",
      "gracias mi ponchin !\n",
      "make them your strength\n",
      "con cari√±o , una canci√≥n , . con√©ctate a la fiesta\n",
      "‚Äú : gracias via x mi para √±a2014 ‚Äù\n",
      "jajajaja yo\n",
      "my brothers are honestly amazing !!! i loved every single one of the presents they gave me !\n",
      "no lmfao i was just not sleepy jajaja .\n",
      "lo mejor hasta ahorita son y mi bella\n",
      "\" : alcoholic ? no , i prefer the term drinking enthusiast . \" jajajajaja accurate ?\n",
      "me maltrato con un guineo .\n",
      "nuestras faldas lucianas en sus diferentes colores para cada personalidad cu√°l prefieres ?\n",
      "obsesionada con este personaje\n",
      "\" que mucha mierda has comido con la monografia esa .. \" pues hasla tu que tal ?\n",
      "belvita !\n",
      "no dejes que el lunes te ponga patas arriba ! empieza la semana con √°nimo . mi\n",
      "para una noticiero estelar es necesario un outfit ! lucete como nuestra querida\n",
      "los amo . ; un besote a mi por confiar en mi y a ustedes tambien .\n",
      "lol okay it 's iaavalos\n",
      "omg this sucks !! i hate being sick can n't even breath :(\n",
      "‚Äú : to err is human , to forgive , devine . ‚Äù\n",
      "exacto\n",
      "your not going to find a girl who loves you more than me\n"
     ]
    }
   ],
   "source": [
    "for chosen_tweet in chosen_tweets:\n",
    "    print (chosen_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T10:23:19.356175Z",
     "start_time": "2021-05-31T10:23:19.352338Z"
    }
   },
   "outputs": [],
   "source": [
    "DISACCENT_MAP = {'√°': 'a', '√©': 'e', '√≠': 'i', '√≥': 'o', '√∫': 'u', '√º': 'u', '√±': 'n'}\n",
    "\n",
    "def drop_accents(tweet: [str], drop_probability: float=1.0) -> [str]:\n",
    "    output = []\n",
    "    for c in tweet:\n",
    "        if c in DISACCENT_MAP and random.random() < drop_probability:\n",
    "            output.append(DISACCENT_MAP[c])\n",
    "        else:\n",
    "            output.append(c)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T10:23:19.361570Z",
     "start_time": "2021-05-31T10:23:19.357955Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jajajaajaja yo odio la palabra \" mocion \" con mi vida entera despues de la estupidez de huelga esa'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = 'jajajaajaja yo odio la palabra \" moci√≥n \" con mi vida entera despu√©s de la estupidez de huelga esa'\n",
    "''.join(drop_accents(example, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T10:23:19.368736Z",
     "start_time": "2021-05-31T10:23:19.363894Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21030"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T10:23:19.382558Z",
     "start_time": "2021-05-31T10:23:19.378676Z"
    }
   },
   "outputs": [],
   "source": [
    "VOWELS_SET = set(\"aeiou√°√©√≠√≥√∫√º\")\n",
    "\n",
    "def drop_vowels(tweet: [str], drop_probability: float=1.0) -> [str]:\n",
    "    output = []\n",
    "    for c in tweet:\n",
    "        if c in VOWELS_SET and random.random() < drop_probability:\n",
    "            continue\n",
    "        output.append(c)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T10:23:19.391057Z",
     "start_time": "2021-05-31T10:23:19.386797Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jajajajaja yo odo la palabr \" moci√≥n \" con m vida entera despu√©s de la estpidez de huelga esa'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(drop_vowels(example, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T10:23:19.396701Z",
     "start_time": "2021-05-31T10:23:19.393597Z"
    }
   },
   "outputs": [],
   "source": [
    "def repeat_vowels(tweet: [str], repeat_probability=0.05, max_repeat=6) -> [str]:\n",
    "    output = []\n",
    "    for c in tweet:\n",
    "        if c in VOWELS_SET and random.random() < repeat_probability:\n",
    "            for _ in range(random.randint(1, max_repeat)):\n",
    "                output.append(c)\n",
    "        output.append(c)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T10:23:19.401608Z",
     "start_time": "2021-05-31T10:23:19.398371Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jajajaajaaaaaaja yo odio la palabra \" moci√≥n \" con miiii vida entera despu√©s de la estupidez dee huelga esa'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(repeat_vowels(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T10:23:19.407308Z",
     "start_time": "2021-05-31T10:23:19.403200Z"
    }
   },
   "outputs": [],
   "source": [
    "CREATIVE_SUB_MAP = {'c': 's', 's': 'z', 'b': 'v', 'v': 'b'}\n",
    "\n",
    "def substitute_creatively(tweet: [str], substitution_probability=0.1):\n",
    "    output = []\n",
    "    for c in tweet:\n",
    "        if c in CREATIVE_SUB_MAP and random.random() < substitution_probability:\n",
    "            output.append(CREATIVE_SUB_MAP[c])\n",
    "        else:\n",
    "            output.append(c)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T10:23:19.412510Z",
     "start_time": "2021-05-31T10:23:19.409273Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jajajaajaja yo odio la palabra \" moci√≥n \" con mi vida entera despu√©s de la estupidez de huelga eza'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(substitute_creatively(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T10:23:19.416619Z",
     "start_time": "2021-05-31T10:23:19.414170Z"
    }
   },
   "outputs": [],
   "source": [
    "def simulate_creative_misspellings(tweet: [str]) -> [str]:\n",
    "    if random.random() < 0.7:\n",
    "        return repeat_vowels(tweet)\n",
    "    else:\n",
    "        return substitute_creatively(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T10:23:19.422381Z",
     "start_time": "2021-05-31T10:23:19.418521Z"
    }
   },
   "outputs": [],
   "source": [
    "def simulate_intentional_shortenings(tweet: [str]) -> [str]:\n",
    "    input_tokens = ''.join(tweet).split(' ')\n",
    "    output_tokens = []\n",
    "    for token in input_tokens:\n",
    "        if random.random() < 0.3:\n",
    "            output_tokens.append(''.join(drop_vowels(list(token), 0.5)))\n",
    "        else:\n",
    "            output_tokens.append(token)\n",
    "    return list(' '.join(output_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T10:23:19.429395Z",
     "start_time": "2021-05-31T10:23:19.425201Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jajajaajaja yo odio l palabra \" moci√≥n \" con mi vida entera despus d la estupidez de huelga esa'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(simulate_intentional_shortenings(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T10:23:19.435747Z",
     "start_time": "2021-05-31T10:23:19.431637Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '√°', '√©', '√≠', '√≥', '√∫', '√º', '√±']\n",
      "{'√º', 'v', 'u', '√°', 's', '√±', 'j', 'b', 'i', 't', '√≥', 'r', '√≠', 'c', 'p', 'l', 'w', 'f', 'y', '√∫', 'm', 'o', 'x', 'g', 'd', 'k', 'h', 'n', 'a', 'e', 'q', 'z', '√©'}\n"
     ]
    }
   ],
   "source": [
    "ALPHABET_L = list(\"abcdefghijklmnopqrstuvwxyz√°√©√≠√≥√∫√º√±\")\n",
    "ALPHABET_S = set(ALPHABET_L)\n",
    "print(ALPHABET_L)\n",
    "print(ALPHABET_S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T10:23:19.443888Z",
     "start_time": "2021-05-31T10:23:19.437538Z"
    }
   },
   "outputs": [],
   "source": [
    "def simulate_bona_fide_spelling_mistakes(tweet: [str], modify_rate=0.1) -> [str]:\n",
    "    # equally add, omit, or substitute each character for a total probability of modify_rate\n",
    "    add_cmf_val = modify_rate / 3\n",
    "    omit_cmf_val = 2 * modify_rate / 3\n",
    "    substitute_cmf_val = modify_rate\n",
    "    \n",
    "    output = []\n",
    "    for c in tweet:\n",
    "        r = random.random()\n",
    "        if r < add_cmf_val:\n",
    "            output.append(c)\n",
    "            output.append(random.choice(ALPHABET_L))\n",
    "        elif r < omit_cmf_val:\n",
    "            continue\n",
    "        elif c in ALPHABET_S and r < substitute_cmf_val:\n",
    "            # we don't apply substitutions to non-alpha\n",
    "            output.append(random.choice(ALPHABET_L))\n",
    "        else:\n",
    "            output.append(c)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T10:23:19.450515Z",
     "start_time": "2021-05-31T10:23:19.446193Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jajajaajaja vo odio la palabra \" moci√≥n \" con mi vida etea despu√©s dte lx esteupidez de huega esa'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(simulate_bona_fide_spelling_mistakes(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T10:23:19.456907Z",
     "start_time": "2021-05-31T10:23:19.453070Z"
    }
   },
   "outputs": [],
   "source": [
    "def identity(tweet: [str]) -> [str]:\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T10:23:19.466785Z",
     "start_time": "2021-05-31T10:23:19.459463Z"
    }
   },
   "outputs": [],
   "source": [
    "DEFAULT_WEIGHT_MAP = [[14, simulate_creative_misspellings],\n",
    "                      [13, drop_accents],\n",
    "                      [10, simulate_intentional_shortenings],\n",
    "                      [8, simulate_bona_fide_spelling_mistakes],\n",
    "                      [10, identity]]\n",
    "\n",
    "class DisjointNoiser:\n",
    "    def __init__(self, weight_map=DEFAULT_WEIGHT_MAP):\n",
    "        self.weight_map = weight_map\n",
    "        total = sum([self.weight_map[i][0] for i in range(len(self.weight_map))])\n",
    "        # we normalize the pmf\n",
    "        for i in range(len(self.weight_map)):\n",
    "            self.weight_map[i][0] /= total\n",
    "        # we make the pmf into a cmf\n",
    "        for i in range(1, len(self.weight_map) - 1):\n",
    "            self.weight_map[i][0] += self.weight_map[i - 1][0]\n",
    "        self.weight_map[-1][0] = 1.0  # so as not to worry about rounding errors\n",
    "\n",
    "    def add_noise(self, tweet: [str]) -> [str]:\n",
    "        \"\"\"Applies at most 1 kind of noising to the tweet according to the weights in the weight map.\n",
    "        Each 'noising' could alter the tweet in multiple places or not at all.\n",
    "        \"\"\"\n",
    "        tweet = list(tweet)\n",
    "        my_random_number = random.random()\n",
    "        for max_prob, noise_function in self.weight_map:\n",
    "            if my_random_number < max_prob:\n",
    "                # print('calling', noise_function)\n",
    "                return noise_function(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T10:23:19.471509Z",
     "start_time": "2021-05-31T10:23:19.468706Z"
    }
   },
   "outputs": [],
   "source": [
    "noiser = DisjointNoiser()\n",
    "for _ in range(10):\n",
    "    noiser.add_noise('abcd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T10:23:19.478736Z",
     "start_time": "2021-05-31T10:23:19.474007Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jjajjaja yo odio la palabra \" moci√≥n \" con mi vida entera despu√©s de la estupidez de huelga esa\n",
      "\n",
      "jajajaajaja yo odio l palabra \" moci√≥n \" con m vida entera dspu√©s de la stpidz de huelga esa\n",
      "\n",
      "jajajaajaja yo odio la palabra \" moci√≥n \" con mi vida entera despu√©s de la estupidez de huelga esa\n",
      "\n",
      "jajajaajaja yo odio la palabra \" mocion \" con mi vida entera despues de la estupidez de huelga esa\n",
      "\n",
      "jajajaajaja yo odio la palabra \" moci√≥n \" con mi vida entera despu√©s de la estupidez de huelga esa\n",
      "\n",
      "jajajaajaja yo odio la palabra \" moci√≥n \" con mi vida entera despu√©s de la estupidez de huelga esa\n",
      "\n",
      "jajajajfja yo odio la ypalabra \" oci√≥n \" con mi vviada etera despu√©s de la estufpidez de huelga qsa\n",
      "\n",
      "jajajaajaja yo odio la palabra \" mci√≥n \" con mi vd entera desps de la estupidez de hulga esa\n",
      "\n",
      "jjajaajajay yo di√° la alabra \" moci√≥n \" con mi vida entera despu√©s de la estoupidez de huelga es\n",
      "\n",
      "jajajaajavja y odiolj plabra \" moci√≥n c\" csn mi visa entera d√±espu√© de la estupidez de helga xsa\n",
      "\n"
     ]
    }
   ],
   "source": [
    "noiser = DisjointNoiser()\n",
    "for _ in range(10):\n",
    "    print(''.join(noiser.add_noise(example)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T10:23:19.485021Z",
     "start_time": "2021-05-31T10:23:19.481432Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21030"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T10:23:19.492793Z",
     "start_time": "2021-05-31T10:23:19.486673Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(len(line) for line in clean_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T10:23:19.499659Z",
     "start_time": "2021-05-31T10:23:19.494513Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49.6810746552544"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(len(line) for line in clean_lines) / len(clean_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T10:23:19.507032Z",
     "start_time": "2021-05-31T10:23:19.501400Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_medium_lines = [line for line in clean_lines if 10 < len(line) < 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T10:23:19.512167Z",
     "start_time": "2021-05-31T10:23:19.508897Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16908"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_medium_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T10:23:19.516569Z",
     "start_time": "2021-05-31T10:23:19.513859Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '√°', '√©', '√≠', '√≥', '√∫', '√º', '√±', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '¬ø', '?', '¬°', '!', '.', ',', ';', '#', ':', '<', '>', '(', ')', \"'\", '‚Äú', '‚Äù', '\"', ' ', '<EOT>', '<PAD>']\n"
     ]
    }
   ],
   "source": [
    "VOCAB_L = ALPHABET_L + list(\"0123456789¬ø?¬°!.,;#:<>()'‚Äú‚Äù\\\" \") + ['<EOT>', '<PAD>']\n",
    "print(VOCAB_L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T10:23:19.522567Z",
     "start_time": "2021-05-31T10:23:19.518410Z"
    }
   },
   "outputs": [],
   "source": [
    "VOCAB_TO_INT = {v: i for i, v in enumerate(VOCAB_L)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T10:23:19.527812Z",
     "start_time": "2021-05-31T10:23:19.524769Z"
    }
   },
   "outputs": [],
   "source": [
    "example_ints = [VOCAB_TO_INT[c] for c in example]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T10:23:19.537826Z",
     "start_time": "2021-05-31T10:23:19.535327Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 0, 9, 0, 9, 0, 0, 9, 0, 9, 0, 60, 24, 14, 60, 14, 3, 8, 14, 60, 11, 0, 60, 15, 0, 11, 0, 1, 17, 0, 60, 59, 60, 12, 14, 2, 8, 29, 13, 60, 59, 60, 2, 14, 13, 60, 12, 8, 60, 21, 8, 3, 0, 60, 4, 13, 19, 4, 17, 0, 60, 3, 4, 18, 15, 20, 27, 18, 60, 3, 4, 60, 11, 0, 60, 4, 18, 19, 20, 15, 8, 3, 4, 25, 60, 3, 4, 60, 7, 20, 4, 11, 6, 0, 60, 4, 18, 0]\n"
     ]
    }
   ],
   "source": [
    "print(example_ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T10:23:19.545077Z",
     "start_time": "2021-05-31T10:23:19.541330Z"
    }
   },
   "outputs": [],
   "source": [
    "def pad_tweet_batch(tweet_batch, max_tweet_length_in_batch):\n",
    "    \"\"\"Pad tweets with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
    "    return [tweet + [VOCAB_TO_INT['<PAD>']] * (max_tweet_length_in_batch - len(tweet)) for tweet in tweet_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T10:23:19.551107Z",
     "start_time": "2021-05-31T10:23:19.547579Z"
    }
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(padded_batch):\n",
    "    encoded_data = np.zeros(\n",
    "        (len(padded_batch), len(padded_batch[0]), len(VOCAB_TO_INT)), dtype=\"float32\"\n",
    "    )\n",
    "    \n",
    "    for i, padded_text in enumerate(padded_batch):\n",
    "        for t, int_value in enumerate(padded_text):\n",
    "            encoded_data[i, t, int_value] = 1.0\n",
    "    \n",
    "    return encoded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T10:23:19.559854Z",
     "start_time": "2021-05-31T10:23:19.553173Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_batches(tweets, noiser, batch_size):\n",
    "    \n",
    "    for batch_i in range(0, len(tweets) // batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        tweets_batch = tweets[start_i:start_i + batch_size]\n",
    "        tweets_batch_noised = [noiser.add_noise(tweet) for tweet in tweets_batch]\n",
    "        \n",
    "        tweets_batch_ints = [[VOCAB_TO_INT[v] for v in tweet] for tweet in tweets_batch]\n",
    "        tweets_batch_noised_ints = [[VOCAB_TO_INT[v] for v in tweet] for tweet in tweets_batch_noised]\n",
    "        \n",
    "        tweets_batch_eot = [tweet + [VOCAB_TO_INT['<EOT>']] for tweet in tweets_batch_ints]\n",
    "        tweets_batch_noised_eot = [tweet + [VOCAB_TO_INT['<EOT>']] for tweet in tweets_batch_noised_ints]\n",
    "        \n",
    "        pad_tweets_batch = np.array(pad_tweet_batch(tweets_batch_eot,\n",
    "                                                    max([len(tweet) for tweet in tweets_batch_eot])))\n",
    "        pad_tweets_noised_batch = np.array(pad_tweet_batch(tweets_batch_noised_eot,\n",
    "                                                           max([len(tweet) for tweet in tweets_batch_noised_eot])))\n",
    "        \n",
    "        # pad_tweets_lengths = [len(tweet) for tweet in pad_tweets_batch]\n",
    "        # pad_tweets_noised_lengths = [len(tweet) for tweet in pad_tweets_noised_batch]\n",
    "        \n",
    "        pad_tweets_encoded_batch = one_hot_encode(pad_tweets_batch)\n",
    "        pad_tweets_noised_encoded_batch = one_hot_encode(pad_tweets_noised_batch)\n",
    "        \n",
    "        # pad_tweets_batch = np.reshape(pad_tweets_batch, (pad_tweets_batch.shape[0], 1, pad_tweets_batch.shape[1]))\n",
    "        # pad_tweets_noised_batch = np.reshape(pad_tweets_noised_batch, (pad_tweets_noised_batch.shape[0], 1, pad_tweets_noised_batch.shape[1]))\n",
    "\n",
    "        # yield pad_tweets_noised_batch, pad_tweets_batch, pad_tweets_noised_lengths, pad_tweets_lengths\n",
    "        # yield pad_tweets_noised_batch, pad_tweets_batch\n",
    "        yield pad_tweets_noised_encoded_batch, pad_tweets_encoded_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T10:23:19.567669Z",
     "start_time": "2021-05-31T10:23:19.561485Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_sorted_medium_lines = sorted(clean_medium_lines, key=lambda t: len(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T10:23:22.242421Z",
     "start_time": "2021-05-31T10:23:22.239839Z"
    }
   },
   "outputs": [],
   "source": [
    "gb = get_batches(clean_sorted_medium_lines, noiser, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T10:23:22.823949Z",
     "start_time": "2021-05-31T10:23:22.808320Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]]],\n",
       "       dtype=float32),\n",
       " array([[[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]]],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T10:23:23.576446Z",
     "start_time": "2021-05-31T10:23:23.574046Z"
    }
   },
   "outputs": [],
   "source": [
    "# pad_tweets_noised_batch, pad_tweets_batch, pad_tweets_noised_lengths, pad_tweets_lengths = next(gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T10:23:25.507564Z",
     "start_time": "2021-05-31T10:23:25.504587Z"
    }
   },
   "outputs": [],
   "source": [
    "pad_tweets_noised_batch, pad_tweets_batch = next(gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T10:23:26.302930Z",
     "start_time": "2021-05-31T10:23:26.293202Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_tweets_noised_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T10:14:34.571653Z",
     "start_time": "2021-05-31T10:14:34.566412Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 1., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 1., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 1., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 1., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 1., 0.]]], dtype=float32)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_tweets_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T10:14:35.064477Z",
     "start_time": "2021-05-31T10:14:35.060759Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(VOCAB_TO_INT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T10:14:35.787031Z",
     "start_time": "2021-05-31T10:14:35.783490Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_TO_INT['a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T10:24:40.685280Z",
     "start_time": "2021-05-31T10:24:40.681721Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_TO_INT['b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T10:24:42.876195Z",
     "start_time": "2021-05-31T10:24:42.873584Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, Dropout, LSTM, RepeatVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T10:29:33.116669Z",
     "start_time": "2021-05-31T10:29:32.992595Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer lstm_13 is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 50)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-161-dc5389b2f4cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLAYERS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     model.add(LSTM(HIDDEN_SIZE, return_sequences=True,\n\u001b[0;32m---> 17\u001b[0;31m                    kernel_initializer=\"he_normal\", dropout=DROPOUT, recurrent_dropout=RECURRENT_DROPOUT))\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVOCAB_TO_INT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"he_normal\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py_3_7_tf_2_5/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    520\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 522\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    523\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py_3_7_tf_2_5/lib/python3.7/site-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    226\u001b[0m       \u001b[0;31m# If the model is being built continuously on top of an input layer:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m       \u001b[0;31m# refresh its output.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m       \u001b[0moutput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSINGLE_LAYER_OUTPUT_ERROR_MSG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py_3_7_tf_2_5/lib/python3.7/site-packages/tensorflow/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m     \u001b[0;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py_3_7_tf_2_5/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    968\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[0;32m--> 970\u001b[0;31m                                                 input_list)\n\u001b[0m\u001b[1;32m    971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m     \u001b[0;31m# Maintains info about the `Layer.call` stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py_3_7_tf_2_5/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[0;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[1;32m   1106\u001b[0m       \u001b[0;31m# Check input assumptions set after layer building, e.g. input shape.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m       outputs = self._keras_tensor_symbolic_call(\n\u001b[0;32m-> 1108\u001b[0;31m           inputs, input_masks, args, kwargs)\n\u001b[0m\u001b[1;32m   1109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1110\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py_3_7_tf_2_5/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_keras_tensor_symbolic_call\u001b[0;34m(self, inputs, input_masks, args, kwargs)\u001b[0m\n\u001b[1;32m    838\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKerasTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_signature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 840\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py_3_7_tf_2_5/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_infer_output_signature\u001b[0;34m(self, inputs, args, kwargs, input_masks)\u001b[0m\n\u001b[1;32m    876\u001b[0m           \u001b[0;31m# overridden).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m           \u001b[0;31m# TODO(kaftan): do we maybe_build here, or have we already done it?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py_3_7_tf_2_5/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2598\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2599\u001b[0m       input_spec.assert_input_compatibility(\n\u001b[0;32m-> 2600\u001b[0;31m           self.input_spec, inputs, self.name)\n\u001b[0m\u001b[1;32m   2601\u001b[0m       \u001b[0minput_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2602\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0minput_list\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype_policy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_dtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py_3_7_tf_2_5/lib/python3.7/site-packages/tensorflow/python/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    217\u001b[0m                          \u001b[0;34m'expected ndim='\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', found ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                          \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'. Full shape received: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m                          str(tuple(shape)))\n\u001b[0m\u001b[1;32m    220\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m       \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 of layer lstm_13 is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 50)"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "DROPOUT = RECURRENT_DROPOUT = 0.3\n",
    "HIDDEN_SIZE = 50\n",
    "\n",
    "model.add(LSTM(units=HIDDEN_SIZE, input_shape=(None, len(VOCAB_TO_INT)),\n",
    "               kernel_initializer=\"he_normal\", dropout=DROPOUT, recurrent_dropout=RECURRENT_DROPOUT))\n",
    "\n",
    "# output_len = 12\n",
    "\n",
    "# model.add(RepeatVector(output_len))\n",
    "\n",
    "LAYERS = 2\n",
    "\n",
    "for _ in range(LAYERS):\n",
    "    model.add(LSTM(HIDDEN_SIZE, return_sequences=True,\n",
    "                   kernel_initializer=\"he_normal\", dropout=DROPOUT, recurrent_dropout=RECURRENT_DROPOUT))\n",
    "\n",
    "model.add(Dense(len(VOCAB_TO_INT), kernel_initializer=\"he_normal\"))\n",
    "\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T10:25:34.109570Z",
     "start_time": "2021-05-31T10:25:34.104588Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_9 (LSTM)                (None, 50)                22800     \n",
      "_________________________________________________________________\n",
      "repeat_vector_3 (RepeatVecto (None, 12, 50)            0         \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 12, 50)            20200     \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 12, 50)            20200     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 12, 63)            3213      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 12, 63)            0         \n",
      "=================================================================\n",
      "Total params: 66,413\n",
      "Trainable params: 66,413\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T10:25:43.923141Z",
     "start_time": "2021-05-31T10:25:36.144812Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fc1a41239e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fc1a41239e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "  71/5636 [..............................] - ETA: 2:53 - loss: 3.6408 - accuracy: 0.1369"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": " logits and labels must be broadcastable: logits_size=[36,63] labels_size=[39,63]\n\t [[node categorical_crossentropy/softmax_cross_entropy_with_logits (defined at <ipython-input-160-4a9fef791c81>:5) ]] [Op:__inference_train_function_40628]\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-160-4a9fef791c81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_sorted_medium_lines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoiser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_sorted_medium_lines\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/py_3_7_tf_2_5/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                 _r=1):\n\u001b[1;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py_3_7_tf_2_5/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py_3_7_tf_2_5/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py_3_7_tf_2_5/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3024\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py_3_7_tf_2_5/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1961\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py_3_7_tf_2_5/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py_3_7_tf_2_5/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m:  logits and labels must be broadcastable: logits_size=[36,63] labels_size=[39,63]\n\t [[node categorical_crossentropy/softmax_cross_entropy_with_logits (defined at <ipython-input-160-4a9fef791c81>:5) ]] [Op:__inference_train_function_40628]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 3\n",
    "\n",
    "batches = get_batches(clean_sorted_medium_lines, noiser, batch_size=BATCH_SIZE)\n",
    "\n",
    "model.fit(batches, steps_per_epoch=len(clean_sorted_medium_lines) // BATCH_SIZE, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
