{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T10:49:14.086919Z",
     "start_time": "2021-05-08T10:49:14.084298Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T10:49:14.093827Z",
     "start_time": "2021-05-08T10:49:14.089231Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_tweet(tweet: str) -> str:\n",
    "    # take out URLs\n",
    "    tweet = re.sub(\"http.*(\\s|$)\", ' ', tweet)\n",
    "    # take out mentions\n",
    "    tweet = re.sub(\"@[^\\s]+\", ' ', tweet)\n",
    "    # take out hashtags\n",
    "    tweet = re.sub(\"#[^\\s]+\", ' ', tweet)\n",
    "    # take out all characters outside of those we enumerate\n",
    "    tweet = re.sub(\"[^\\da-zA-Z√°√©√≠√≥√∫√º√±√Å√â√ç√ë√ì√ö√ú¬ø?¬°!.,;#:<>()'‚Äú‚Äù\\\"\\s]\", ' ', tweet)\n",
    "    # lowercase everything\n",
    "    tweet = tweet.lower()\n",
    "    # reset spaces\n",
    "    tweet = re.sub(\"\\s+\", ' ', tweet)\n",
    "    tweet = re.sub(\"^\\s+\", '', tweet)\n",
    "    tweet = re.sub(\"\\s$\", '', tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T10:49:14.112244Z",
     "start_time": "2021-05-08T10:49:14.096186Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El vlog de hoy , peque√±as sorpresas en el correo : omg GORRAS !! https://t.co/crJzDhip7O\n",
      "el vlog de hoy , peque√±as sorpresas en el correo : omg gorras !!\n",
      "@ritacm01 probablemente üåù\n",
      "probablemente\n",
      "√±eta hice eye contact con la profesora y me mand√≥ a hablar\n",
      "√±eta hice eye contact con la profesora y me mand√≥ a hablar\n",
      "ESTE CABRON ME LA EXPLOTA CADA VEZ QUE SE METE DONDE NADIE LO FUCKING LLAMA !!\n",
      "este cabron me la explota cada vez que se mete donde nadie lo fucking llama !!\n",
      "@Jona_Boy13 vengo por el weewee\n",
      "vengo por el weewee\n",
      "Ok yaaa enough Duranguense tweets 4 2day a llamarle a Mi Reina Hermosa hija d mi pinshi suegra lol k su voz es lo k me completa Mis Noches ‚ô•\n",
      "ok yaaa enough duranguense tweets 4 2day a llamarle a mi reina hermosa hija d mi pinshi suegra lol k su voz es lo k me completa mis noches\n",
      "been wahcing cops for a minute now haaha\n",
      "been wahcing cops for a minute now haaha\n",
      "Held accountable : Ser responsable/ Hacerse responsable . I do n't want to be held accountable for your bad actions .\n",
      "held accountable : ser responsable hacerse responsable . i do n't want to be held accountable for your bad actions .\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# spotchecking loop to check that clean_tweet() works\n",
    "with open('lid_train_lines.txt', 'r') as input_file:\n",
    "    for line in input_file:\n",
    "        if random.random() < 0.0005:\n",
    "            line = line.strip()\n",
    "            print(line)\n",
    "            print(clean_tweet(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T10:49:14.388117Z",
     "start_time": "2021-05-08T10:49:14.114149Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_lines = []\n",
    "\n",
    "with open('lid_train_lines.txt', 'r') as input_file:\n",
    "    for line in input_file:\n",
    "        clean_lines.append(clean_tweet(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T10:49:14.393229Z",
     "start_time": "2021-05-08T10:49:14.389965Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21030"
      ]
     },
     "execution_count": 541,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T10:49:14.397349Z",
     "start_time": "2021-05-08T10:49:14.394980Z"
    }
   },
   "outputs": [],
   "source": [
    "random.seed(a=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T10:49:14.401875Z",
     "start_time": "2021-05-08T10:49:14.399102Z"
    }
   },
   "outputs": [],
   "source": [
    "chosen_tweets = random.choices(clean_lines, k=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T10:49:14.410010Z",
     "start_time": "2021-05-08T10:49:14.405362Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ma√±ana viernes wilfrido vargas ..',\n",
       " 'u love me',\n",
       " 'so original .. .',\n",
       " 'whats up with nature',\n",
       " 'aguanta tu ritmo .',\n",
       " \"i do n't like ( xxx ) people , they always think they 're right and do n't listen to anything else . ( testarudo a obstinado a tercho a )\",\n",
       " 'ni pinches moscas pescas tu mondaooo',\n",
       " 'new week , be kind and sweet ! vestido sahara',\n",
       " 'no puedo bregar con la gente stalker',\n",
       " 'a cantarle las ma√±anitas ala virgencita',\n",
       " 'el vlog de hoy aunque sea tarde , peque√±as cenas agradables : el d√≠a m√°s feliz de mi vida',\n",
       " 'all bad',\n",
       " 'no es sin√≥nimo pero significa lo mismo , i get out of work at 6 pm , creo q esto es lo q quer√≠as decir :)',\n",
       " 'blusa violeta y sus diferentes estampados ! escoge tu color favorito made in ecuador',\n",
       " '¬°¬° nuevo v√≠deo !! en esta ocasi√≥n , un famoso presentador de tv me hace una dur√≠sima entrevista .. . ¬° se .. .',\n",
       " 'get the hang of it : acostumbrarse a esto poder dominarlo person 1 : this new job is so stressful . person 2 : you ll get the hang of it .',\n",
       " 'clase de text',\n",
       " 'voy a empezar a pedirle trabajos a la gente como me los piden a mi',\n",
       " 'en estos momentos prefiero hacer mi ensayo de psicolog√≠a que trabajar',\n",
       " 'online class de sexual harassment .. . arianna is so mean',\n",
       " '\" y nunca me entretengo aber si me aman , les doy mi corazon por una semana , y luego dejo k se alejen de mi si les da la gana \"',\n",
       " \"ruth likes my dad 's new truck\",\n",
       " '‚Äú : si el camino es dif√≠cil .. . normalmente es porque vas en la direcci√≥n correcta . ‚Äù very true !! !',\n",
       " 'been waiting haha',\n",
       " 'lol jk but gracias corazon',\n",
       " '\" : my best friend',\n",
       " 'gracias flaca y el vestido es de la marca .',\n",
       " 'haber saludao !',\n",
       " 'u love me',\n",
       " 'unless they cut my hours again and tell me bout to go in today',\n",
       " 'es fun ver lo que dicen y eso jaja .',\n",
       " 'this bop put bacon on my food omfg foh',\n",
       " '',\n",
       " 'yo no pregunte pq a la defensiva ?',\n",
       " 'comenz√≥ la recta final a 1 mes y 1 d√≠a !! ! omg : o',\n",
       " 'la respuesta de es :',\n",
       " 'tu sabes que no',\n",
       " 'a escucharlo y disfrutarlo ! ( si te gusta claro )',\n",
       " 'te relacionas ? jajaja',\n",
       " '‚Äú : i miss my favorite stylist ! ‚Äù miss u to chica',\n",
       " 'ay y yo , pero tengo el estomago vacio y me iria over',\n",
       " 'si quieren un poco de inspiracion busquen el poema de steve jobs \" here \\'s to the crazy ones \"',\n",
       " 'fuhk valley is gunna close already',\n",
       " 'so si te van aser ?',\n",
       " 'a la verdad que el verano despues de freshmen year yo estaba obesa',\n",
       " \"plain and simple . simple y llanamente . i do n t need a new car , i 'm not interested , plain and simple\",\n",
       " 'as√≠ o m√°s energ√≠a ? la mayor cantidad de m√∫sica',\n",
       " 'that was so emberrasing',\n",
       " 'lol your all bad',\n",
       " 'tfti !! i want chokomil lmao',\n",
       " 'mi perrita abda de latosa y no me deja dormir .',\n",
       " 'me estan comprando una mac',\n",
       " 'a tus ni√±as les gusts el mundo del modelaje ? registralas en imagen modeling by la',\n",
       " \"yessss !! it does :d nd it 's gonna help us muajajaja . ;)\",\n",
       " 'gastar toel dinero que hiciste ?',\n",
       " 'buen provecho !',\n",
       " 'todos ayer jajaja',\n",
       " 'me metieron con un guineo en la cara ! jajajajjaj',\n",
       " 'quisiera volver a ser peque√±o , no pagar renta , ni usar desodrante .',\n",
       " '‚Äú : que guapa esperemos pronto verte por mexico saludos de todo el club de hp ‚Äù gracias chica . saludos a todos',\n",
       " 'go with the flow : varias cosas una sigue a la mayor√≠a p1 : are we going to plan the trip ? p2 : let s just go with the flow and see what happen',\n",
       " \"jajajajja ayy precious have you tried the chocolate filled gansitos ?? they 're sooooo good !!!\",\n",
       " 'la he escuchado mucho , pero no la usaba :)',\n",
       " 'can you ? lol',\n",
       " 'tengo el brazo como el muslo de beyonc√© , pero totally worth it . buenos d√≠as .',\n",
       " 'hate being so detailed when describing a person because i hit their qualities spot on when they describe me they make me sound so blah .',\n",
       " 'really what is it about ? i wanted too go see it lmao',\n",
       " 'all my tios are coming to my house .... like always',\n",
       " 'jajajajaja mira ponte a estudiar para el depa de ma√±ana y shh',\n",
       " 'life long dream ? ir a un concierto de coldplay',\n",
       " 'beffas beers pool',\n",
       " 'imagen modeling by la gatita reg√≠strate en',\n",
       " 'la primera es la correcta , no me expliqu√© bien :( can you tell me who she is , can you tell me where it is , do you know who he is :)',\n",
       " 'no',\n",
       " 'hay alguien que o no hablo en todo el dia o tiene la mente a millon',\n",
       " 'down like a real friends supposed to im trying to show you the life of somebody like you should be living',\n",
       " 'gracias',\n",
       " \"oh no ! let 's figure this out , edgar . could you please follow us and send us a dm for further assistance ? thanks ! mh\",\n",
       " 'gracias mi ponchin !',\n",
       " 'make them your strength',\n",
       " 'con cari√±o , una canci√≥n , . con√©ctate a la fiesta',\n",
       " '‚Äú : gracias via x mi para √±a2014 ‚Äù',\n",
       " 'jajajaja yo',\n",
       " 'my brothers are honestly amazing !!! i loved every single one of the presents they gave me !',\n",
       " 'no lmfao i was just not sleepy jajaja .',\n",
       " 'lo mejor hasta ahorita son y mi bella',\n",
       " '\" : alcoholic ? no , i prefer the term drinking enthusiast . \" jajajajaja accurate ?',\n",
       " 'me maltrato con un guineo .',\n",
       " 'nuestras faldas lucianas en sus diferentes colores para cada personalidad cu√°l prefieres ?',\n",
       " 'obsesionada con este personaje',\n",
       " '\" que mucha mierda has comido con la monografia esa .. \" pues hasla tu que tal ?',\n",
       " 'belvita !',\n",
       " 'no dejes que el lunes te ponga patas arriba ! empieza la semana con √°nimo . mi',\n",
       " 'para una noticiero estelar es necesario un outfit ! lucete como nuestra querida',\n",
       " 'los amo . ; un besote a mi por confiar en mi y a ustedes tambien .',\n",
       " \"lol okay it 's iaavalos\",\n",
       " \"omg this sucks !! i hate being sick can n't even breath :(\",\n",
       " '‚Äú : to err is human , to forgive , devine . ‚Äù',\n",
       " 'exacto',\n",
       " 'your not going to find a girl who loves you more than me']"
      ]
     },
     "execution_count": 544,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chosen_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T10:49:14.417030Z",
     "start_time": "2021-05-08T10:49:14.412462Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ma√±ana viernes wilfrido vargas ..\n",
      "u love me\n",
      "so original .. .\n",
      "whats up with nature\n",
      "aguanta tu ritmo .\n",
      "i do n't like ( xxx ) people , they always think they 're right and do n't listen to anything else . ( testarudo a obstinado a tercho a )\n",
      "ni pinches moscas pescas tu mondaooo\n",
      "new week , be kind and sweet ! vestido sahara\n",
      "no puedo bregar con la gente stalker\n",
      "a cantarle las ma√±anitas ala virgencita\n",
      "el vlog de hoy aunque sea tarde , peque√±as cenas agradables : el d√≠a m√°s feliz de mi vida\n",
      "all bad\n",
      "no es sin√≥nimo pero significa lo mismo , i get out of work at 6 pm , creo q esto es lo q quer√≠as decir :)\n",
      "blusa violeta y sus diferentes estampados ! escoge tu color favorito made in ecuador\n",
      "¬°¬° nuevo v√≠deo !! en esta ocasi√≥n , un famoso presentador de tv me hace una dur√≠sima entrevista .. . ¬° se .. .\n",
      "get the hang of it : acostumbrarse a esto poder dominarlo person 1 : this new job is so stressful . person 2 : you ll get the hang of it .\n",
      "clase de text\n",
      "voy a empezar a pedirle trabajos a la gente como me los piden a mi\n",
      "en estos momentos prefiero hacer mi ensayo de psicolog√≠a que trabajar\n",
      "online class de sexual harassment .. . arianna is so mean\n",
      "\" y nunca me entretengo aber si me aman , les doy mi corazon por una semana , y luego dejo k se alejen de mi si les da la gana \"\n",
      "ruth likes my dad 's new truck\n",
      "‚Äú : si el camino es dif√≠cil .. . normalmente es porque vas en la direcci√≥n correcta . ‚Äù very true !! !\n",
      "been waiting haha\n",
      "lol jk but gracias corazon\n",
      "\" : my best friend\n",
      "gracias flaca y el vestido es de la marca .\n",
      "haber saludao !\n",
      "u love me\n",
      "unless they cut my hours again and tell me bout to go in today\n",
      "es fun ver lo que dicen y eso jaja .\n",
      "this bop put bacon on my food omfg foh\n",
      "\n",
      "yo no pregunte pq a la defensiva ?\n",
      "comenz√≥ la recta final a 1 mes y 1 d√≠a !! ! omg : o\n",
      "la respuesta de es :\n",
      "tu sabes que no\n",
      "a escucharlo y disfrutarlo ! ( si te gusta claro )\n",
      "te relacionas ? jajaja\n",
      "‚Äú : i miss my favorite stylist ! ‚Äù miss u to chica\n",
      "ay y yo , pero tengo el estomago vacio y me iria over\n",
      "si quieren un poco de inspiracion busquen el poema de steve jobs \" here 's to the crazy ones \"\n",
      "fuhk valley is gunna close already\n",
      "so si te van aser ?\n",
      "a la verdad que el verano despues de freshmen year yo estaba obesa\n",
      "plain and simple . simple y llanamente . i do n t need a new car , i 'm not interested , plain and simple\n",
      "as√≠ o m√°s energ√≠a ? la mayor cantidad de m√∫sica\n",
      "that was so emberrasing\n",
      "lol your all bad\n",
      "tfti !! i want chokomil lmao\n",
      "mi perrita abda de latosa y no me deja dormir .\n",
      "me estan comprando una mac\n",
      "a tus ni√±as les gusts el mundo del modelaje ? registralas en imagen modeling by la\n",
      "yessss !! it does :d nd it 's gonna help us muajajaja . ;)\n",
      "gastar toel dinero que hiciste ?\n",
      "buen provecho !\n",
      "todos ayer jajaja\n",
      "me metieron con un guineo en la cara ! jajajajjaj\n",
      "quisiera volver a ser peque√±o , no pagar renta , ni usar desodrante .\n",
      "‚Äú : que guapa esperemos pronto verte por mexico saludos de todo el club de hp ‚Äù gracias chica . saludos a todos\n",
      "go with the flow : varias cosas una sigue a la mayor√≠a p1 : are we going to plan the trip ? p2 : let s just go with the flow and see what happen\n",
      "jajajajja ayy precious have you tried the chocolate filled gansitos ?? they 're sooooo good !!!\n",
      "la he escuchado mucho , pero no la usaba :)\n",
      "can you ? lol\n",
      "tengo el brazo como el muslo de beyonc√© , pero totally worth it . buenos d√≠as .\n",
      "hate being so detailed when describing a person because i hit their qualities spot on when they describe me they make me sound so blah .\n",
      "really what is it about ? i wanted too go see it lmao\n",
      "all my tios are coming to my house .... like always\n",
      "jajajajaja mira ponte a estudiar para el depa de ma√±ana y shh\n",
      "life long dream ? ir a un concierto de coldplay\n",
      "beffas beers pool\n",
      "imagen modeling by la gatita reg√≠strate en\n",
      "la primera es la correcta , no me expliqu√© bien :( can you tell me who she is , can you tell me where it is , do you know who he is :)\n",
      "no\n",
      "hay alguien que o no hablo en todo el dia o tiene la mente a millon\n",
      "down like a real friends supposed to im trying to show you the life of somebody like you should be living\n",
      "gracias\n",
      "oh no ! let 's figure this out , edgar . could you please follow us and send us a dm for further assistance ? thanks ! mh\n",
      "gracias mi ponchin !\n",
      "make them your strength\n",
      "con cari√±o , una canci√≥n , . con√©ctate a la fiesta\n",
      "‚Äú : gracias via x mi para √±a2014 ‚Äù\n",
      "jajajaja yo\n",
      "my brothers are honestly amazing !!! i loved every single one of the presents they gave me !\n",
      "no lmfao i was just not sleepy jajaja .\n",
      "lo mejor hasta ahorita son y mi bella\n",
      "\" : alcoholic ? no , i prefer the term drinking enthusiast . \" jajajajaja accurate ?\n",
      "me maltrato con un guineo .\n",
      "nuestras faldas lucianas en sus diferentes colores para cada personalidad cu√°l prefieres ?\n",
      "obsesionada con este personaje\n",
      "\" que mucha mierda has comido con la monografia esa .. \" pues hasla tu que tal ?\n",
      "belvita !\n",
      "no dejes que el lunes te ponga patas arriba ! empieza la semana con √°nimo . mi\n",
      "para una noticiero estelar es necesario un outfit ! lucete como nuestra querida\n",
      "los amo . ; un besote a mi por confiar en mi y a ustedes tambien .\n",
      "lol okay it 's iaavalos\n",
      "omg this sucks !! i hate being sick can n't even breath :(\n",
      "‚Äú : to err is human , to forgive , devine . ‚Äù\n",
      "exacto\n",
      "your not going to find a girl who loves you more than me\n"
     ]
    }
   ],
   "source": [
    "for chosen_tweet in chosen_tweets:\n",
    "    print (chosen_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T10:49:14.422320Z",
     "start_time": "2021-05-08T10:49:14.418660Z"
    }
   },
   "outputs": [],
   "source": [
    "DISACCENT_MAP = {'√°': 'a', '√©': 'e', '√≠': 'i', '√≥': 'o', '√∫': 'u', '√º': 'u', '√±': 'n'}\n",
    "\n",
    "def drop_accents(tweet: [str], drop_probability: float=1.0) -> [str]:\n",
    "    output = []\n",
    "    for c in tweet:\n",
    "        if c in DISACCENT_MAP and random.random() < drop_probability:\n",
    "            output.append(DISACCENT_MAP[c])\n",
    "        else:\n",
    "            output.append(c)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T10:49:14.427421Z",
     "start_time": "2021-05-08T10:49:14.423962Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jajajaajaja yo odio la palabra \" mocion \" con mi vida entera despues de la estupidez de huelga esa'"
      ]
     },
     "execution_count": 547,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = 'jajajaajaja yo odio la palabra \" moci√≥n \" con mi vida entera despu√©s de la estupidez de huelga esa'\n",
    "''.join(drop_accents(example, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T10:49:14.433669Z",
     "start_time": "2021-05-08T10:49:14.429418Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21030"
      ]
     },
     "execution_count": 548,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T10:49:14.439880Z",
     "start_time": "2021-05-08T10:49:14.435771Z"
    }
   },
   "outputs": [],
   "source": [
    "VOWELS_SET = set(\"aeiou√°√©√≠√≥√∫√º\")\n",
    "\n",
    "def drop_vowels(tweet: [str], drop_probability: float=1.0) -> [str]:\n",
    "    output = []\n",
    "    for c in tweet:\n",
    "        if c in VOWELS_SET and random.random() < drop_probability:\n",
    "            continue\n",
    "        output.append(c)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T10:49:14.446704Z",
     "start_time": "2021-05-08T10:49:14.442144Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jajajajaja yo odo la palabr \" moci√≥n \" con m vida entera despu√©s de la estpidez de huelga esa'"
      ]
     },
     "execution_count": 550,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(drop_vowels(example, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T10:49:14.452813Z",
     "start_time": "2021-05-08T10:49:14.449195Z"
    }
   },
   "outputs": [],
   "source": [
    "def repeat_vowels(tweet: [str], repeat_probability=0.05, max_repeat=6) -> [str]:\n",
    "    output = []\n",
    "    for c in tweet:\n",
    "        if c in VOWELS_SET and random.random() < repeat_probability:\n",
    "            for _ in range(random.randint(1, max_repeat)):\n",
    "                output.append(c)\n",
    "        output.append(c)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T10:49:14.457587Z",
     "start_time": "2021-05-08T10:49:14.454504Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jajajaajaaaaaaja yo odio la palabra \" moci√≥n \" con miiii vida entera despu√©s de la estupidez dee huelga esa'"
      ]
     },
     "execution_count": 552,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(repeat_vowels(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T10:49:14.463651Z",
     "start_time": "2021-05-08T10:49:14.459627Z"
    }
   },
   "outputs": [],
   "source": [
    "CREATIVE_SUB_MAP = {'c': 's', 's': 'z', 'b': 'v', 'v': 'b'}\n",
    "\n",
    "def substitute_creatively(tweet: [str], substitution_probability=0.1):\n",
    "    output = []\n",
    "    for c in tweet:\n",
    "        if c in CREATIVE_SUB_MAP and random.random() < substitution_probability:\n",
    "            output.append(CREATIVE_SUB_MAP[c])\n",
    "        else:\n",
    "            output.append(c)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T10:49:14.468775Z",
     "start_time": "2021-05-08T10:49:14.465339Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jajajaajaja yo odio la palabra \" moci√≥n \" con mi vida entera despu√©s de la estupidez de huelga eza'"
      ]
     },
     "execution_count": 554,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(substitute_creatively(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T10:49:14.473351Z",
     "start_time": "2021-05-08T10:49:14.470328Z"
    }
   },
   "outputs": [],
   "source": [
    "def simulate_creative_misspellings(tweet: [str]) -> [str]:\n",
    "    if random.random() < 0.7:\n",
    "        return repeat_vowels(tweet)\n",
    "    else:\n",
    "        return substitute_creatively(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T10:49:14.480642Z",
     "start_time": "2021-05-08T10:49:14.475462Z"
    }
   },
   "outputs": [],
   "source": [
    "def simulate_intentional_shortenings(tweet: [str]) -> [str]:\n",
    "    input_tokens = ''.join(tweet).split(' ')\n",
    "    output_tokens = []\n",
    "    for token in input_tokens:\n",
    "        if random.random() < 0.3:\n",
    "            output_tokens.append(''.join(drop_vowels(list(token), 0.5)))\n",
    "        else:\n",
    "            output_tokens.append(token)\n",
    "    return list(' '.join(output_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T10:49:14.487961Z",
     "start_time": "2021-05-08T10:49:14.483544Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jajajaajaja yo odio l palabra \" moci√≥n \" con mi vida entera despus d la estupidez de huelga esa'"
      ]
     },
     "execution_count": 557,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(simulate_intentional_shortenings(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T10:49:14.494411Z",
     "start_time": "2021-05-08T10:49:14.489955Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '√°', '√©', '√≠', '√≥', '√∫', '√º', '√±']\n",
      "{'j', 'h', 'b', '√≥', '√±', 'g', 'm', '√≠', '√°', 'd', 'x', '√º', 'p', 'c', 'e', 'v', 'o', 'l', 'n', '√©', 'f', 't', '√∫', 'k', 'i', 'r', 'a', 'w', 'y', 'u', 'q', 's', 'z'}\n"
     ]
    }
   ],
   "source": [
    "ALPHABET_L = list(\"abcdefghijklmnopqrstuvwxyz√°√©√≠√≥√∫√º√±\")\n",
    "ALPHABET_S = set(ALPHABET_L)\n",
    "print(ALPHABET_L)\n",
    "print(ALPHABET_S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T10:49:14.505093Z",
     "start_time": "2021-05-08T10:49:14.500244Z"
    }
   },
   "outputs": [],
   "source": [
    "def simulate_bona_fide_spelling_mistakes(tweet: [str], modify_rate=0.1) -> [str]:\n",
    "    # equally add, omit, or substitute each character for a total probability of modify_rate\n",
    "    add_cmf_val = modify_rate / 3\n",
    "    omit_cmf_val = 2 * modify_rate / 3\n",
    "    substitute_cmf_val = modify_rate\n",
    "    \n",
    "    output = []\n",
    "    for c in tweet:\n",
    "        r = random.random()\n",
    "        if r < add_cmf_val:\n",
    "            output.append(c)\n",
    "            output.append(random.choice(ALPHABET_L))\n",
    "        elif r < omit_cmf_val:\n",
    "            continue\n",
    "        elif c in ALPHABET_S and r < substitute_cmf_val:\n",
    "            # we don't apply substitutions to non-alpha\n",
    "            output.append(random.choice(ALPHABET_L))\n",
    "        else:\n",
    "            output.append(c)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T10:49:14.515214Z",
     "start_time": "2021-05-08T10:49:14.510101Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jajajaajaja vo odio la palabra \" moci√≥n \" con mi vida etea despu√©s dte lx esteupidez de huega esa'"
      ]
     },
     "execution_count": 560,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(simulate_bona_fide_spelling_mistakes(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T10:49:14.519601Z",
     "start_time": "2021-05-08T10:49:14.517013Z"
    }
   },
   "outputs": [],
   "source": [
    "def identity(tweet: [str]) -> [str]:\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T10:49:14.529477Z",
     "start_time": "2021-05-08T10:49:14.521877Z"
    }
   },
   "outputs": [],
   "source": [
    "DEFAULT_WEIGHT_MAP = [[14, simulate_creative_misspellings],\n",
    "                      [13, drop_accents],\n",
    "                      [10, simulate_intentional_shortenings],\n",
    "                      [8, simulate_bona_fide_spelling_mistakes],\n",
    "                      [10, identity]]\n",
    "\n",
    "class DisjointNoiser:\n",
    "    def __init__(self, weight_map=DEFAULT_WEIGHT_MAP):\n",
    "        self.weight_map = weight_map\n",
    "        total = sum([self.weight_map[i][0] for i in range(len(self.weight_map))])\n",
    "        # we normalize the pmf\n",
    "        for i in range(len(self.weight_map)):\n",
    "            self.weight_map[i][0] /= total\n",
    "        # we make the pmf into a cmf\n",
    "        for i in range(1, len(self.weight_map) - 1):\n",
    "            self.weight_map[i][0] += self.weight_map[i - 1][0]\n",
    "        self.weight_map[-1][0] = 1.0  # so as not to worry about rounding errors\n",
    "\n",
    "    def add_noise(self, tweet: [str]) -> [str]:\n",
    "        \"\"\"Applies at most 1 kind of noising to the tweet according to the weights in the weight map.\n",
    "        Each 'noising' could alter the tweet in multiple places or not at all.\n",
    "        \"\"\"\n",
    "        tweet = list(tweet)\n",
    "        my_random_number = random.random()\n",
    "        for max_prob, noise_function in self.weight_map:\n",
    "            if my_random_number < max_prob:\n",
    "                # print('calling', noise_function)\n",
    "                return noise_function(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T10:49:14.533776Z",
     "start_time": "2021-05-08T10:49:14.531357Z"
    }
   },
   "outputs": [],
   "source": [
    "noiser = DisjointNoiser()\n",
    "for _ in range(10):\n",
    "    noiser.add_noise('abcd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T10:49:14.539716Z",
     "start_time": "2021-05-08T10:49:14.535617Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jjajjaja yo odio la palabra \" moci√≥n \" con mi vida entera despu√©s de la estupidez de huelga esa\n",
      "\n",
      "jajajaajaja yo odio l palabra \" moci√≥n \" con m vida entera dspu√©s de la stpidz de huelga esa\n",
      "\n",
      "jajajaajaja yo odio la palabra \" moci√≥n \" con mi vida entera despu√©s de la estupidez de huelga esa\n",
      "\n",
      "jajajaajaja yo odio la palabra \" mocion \" con mi vida entera despues de la estupidez de huelga esa\n",
      "\n",
      "jajajaajaja yo odio la palabra \" moci√≥n \" con mi vida entera despu√©s de la estupidez de huelga esa\n",
      "\n",
      "jajajaajaja yo odio la palabra \" moci√≥n \" con mi vida entera despu√©s de la estupidez de huelga esa\n",
      "\n",
      "jajajajfja yo odio la ypalabra \" oci√≥n \" con mi vviada etera despu√©s de la estufpidez de huelga qsa\n",
      "\n",
      "jajajaajaja yo odio la palabra \" mci√≥n \" con mi vd entera desps de la estupidez de hulga esa\n",
      "\n",
      "jjajaajajay yo di√° la alabra \" moci√≥n \" con mi vida entera despu√©s de la estoupidez de huelga es\n",
      "\n",
      "jajajaajavja y odiolj plabra \" moci√≥n c\" csn mi visa entera d√±espu√© de la estupidez de helga xsa\n",
      "\n"
     ]
    }
   ],
   "source": [
    "noiser = DisjointNoiser()\n",
    "for _ in range(10):\n",
    "    print(''.join(noiser.add_noise(example)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T10:49:14.546697Z",
     "start_time": "2021-05-08T10:49:14.542125Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21030"
      ]
     },
     "execution_count": 565,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T10:49:14.557731Z",
     "start_time": "2021-05-08T10:49:14.548999Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154"
      ]
     },
     "execution_count": 566,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(len(line) for line in clean_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T10:49:14.566773Z",
     "start_time": "2021-05-08T10:49:14.560170Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49.6810746552544"
      ]
     },
     "execution_count": 567,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(len(line) for line in clean_lines) / len(clean_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T10:49:14.575873Z",
     "start_time": "2021-05-08T10:49:14.568494Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_medium_lines = [line for line in clean_lines if 10 < len(line) < 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T10:49:14.582478Z",
     "start_time": "2021-05-08T10:49:14.578561Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16908"
      ]
     },
     "execution_count": 569,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_medium_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T10:49:14.588120Z",
     "start_time": "2021-05-08T10:49:14.584731Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '√°', '√©', '√≠', '√≥', '√∫', '√º', '√±', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '¬ø', '?', '¬°', '!', '.', ',', ';', '#', ':', '<', '>', '(', ')', \"'\", '‚Äú', '‚Äù', '\"', ' ', '<EOT>', '<PAD>']\n"
     ]
    }
   ],
   "source": [
    "VOCAB_L = ALPHABET_L + list(\"0123456789¬ø?¬°!.,;#:<>()'‚Äú‚Äù\\\" \") + ['<EOT>', '<PAD>']\n",
    "print(VOCAB_L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T10:49:14.592267Z",
     "start_time": "2021-05-08T10:49:14.589746Z"
    }
   },
   "outputs": [],
   "source": [
    "VOCAB_TO_INT = {v: i for i, v in enumerate(VOCAB_L)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T10:49:14.597573Z",
     "start_time": "2021-05-08T10:49:14.594955Z"
    }
   },
   "outputs": [],
   "source": [
    "example_ints = [VOCAB_TO_INT[c] for c in example]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T10:49:14.601780Z",
     "start_time": "2021-05-08T10:49:14.599344Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 0, 9, 0, 9, 0, 0, 9, 0, 9, 0, 60, 24, 14, 60, 14, 3, 8, 14, 60, 11, 0, 60, 15, 0, 11, 0, 1, 17, 0, 60, 59, 60, 12, 14, 2, 8, 29, 13, 60, 59, 60, 2, 14, 13, 60, 12, 8, 60, 21, 8, 3, 0, 60, 4, 13, 19, 4, 17, 0, 60, 3, 4, 18, 15, 20, 27, 18, 60, 3, 4, 60, 11, 0, 60, 4, 18, 19, 20, 15, 8, 3, 4, 25, 60, 3, 4, 60, 7, 20, 4, 11, 6, 0, 60, 4, 18, 0]\n"
     ]
    }
   ],
   "source": [
    "print(example_ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T10:49:14.606697Z",
     "start_time": "2021-05-08T10:49:14.603488Z"
    }
   },
   "outputs": [],
   "source": [
    "def pad_tweet_batch(tweet_batch, max_tweet_length_in_batch):\n",
    "    \"\"\"Pad tweets with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
    "    return [tweet + [VOCAB_TO_INT['<PAD>']] * (max_tweet_length_in_batch - len(tweet)) for tweet in tweet_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T11:46:00.444681Z",
     "start_time": "2021-05-08T11:46:00.438162Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_batches(tweets, noiser, batch_size):\n",
    "    \n",
    "    for batch_i in range(0, len(tweets) // batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        tweets_batch = tweets[start_i:start_i + batch_size]\n",
    "        tweets_batch_noised = [noiser.add_noise(tweet) for tweet in tweets_batch]\n",
    "        \n",
    "        tweets_batch_ints = [[VOCAB_TO_INT[v] for v in tweet] for tweet in tweets_batch]\n",
    "        tweets_batch_noised_ints = [[VOCAB_TO_INT[v] for v in tweet] for tweet in tweets_batch_noised]\n",
    "        \n",
    "        tweets_batch_eot = [tweet + [VOCAB_TO_INT['<EOT>']] for tweet in tweets_batch_ints]\n",
    "        tweets_batch_noised_eot = [tweet + [VOCAB_TO_INT['<EOT>']] for tweet in tweets_batch_noised_ints]\n",
    "        \n",
    "        pad_tweets_batch = np.array(pad_tweet_batch(tweets_batch_eot,\n",
    "                                                    max([len(tweet) for tweet in tweets_batch_eot])))\n",
    "        pad_tweets_noised_batch = np.array(pad_tweet_batch(tweets_batch_noised_eot,\n",
    "                                                           max([len(tweet) for tweet in tweets_batch_noised_eot])))\n",
    "        \n",
    "        pad_tweets_lengths = [len(tweet) for tweet in pad_tweets_batch]\n",
    "        pad_tweets_noised_lengths = [len(tweet) for tweet in pad_tweets_noised_batch]\n",
    "        \n",
    "        # yield(pad_tweets_noised_batch, pad_tweets_batch, pad_tweets_noised_lengths, pad_tweets_lengths)\n",
    "        yield [pad_tweets_noised_batch, pad_tweets_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T11:46:00.799187Z",
     "start_time": "2021-05-08T11:46:00.791498Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_sorted_medium_lines = sorted(clean_medium_lines, key=lambda t: len(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T11:46:01.124325Z",
     "start_time": "2021-05-08T11:46:01.121593Z"
    }
   },
   "outputs": [],
   "source": [
    "gb = get_batches(clean_sorted_medium_lines, noiser, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T11:46:01.443204Z",
     "start_time": "2021-05-08T11:46:01.439991Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 2, 20,  0, 13,  3, 14, 60, 13, 14, 60, 44, 61],\n",
      "       [ 6, 17,  0,  2,  8,  0, 18, 18, 60, 51, 55, 61],\n",
      "       [ 9,  0,  8,  0,  9,  0,  9,  0,  9,  9,  0, 61],\n",
      "       [ 5,  0, 11, 19,  0,  1,  0, 18, 60, 19, 20, 61],\n",
      "       [13,  8, 60,  8,  3,  4,  0, 60, 46, 60, 51, 61],\n",
      "       [ 3, 14, 22, 13, 60,  9,  0,  9,  0,  9,  0, 61],\n",
      "       [14, 13, 19,  0,  0,  0, 60,  9,  0,  9,  0, 61],\n",
      "       [19,  0, 10,  4, 60,  0, 60,  7,  8, 13, 19, 61]]), array([[ 2, 20,  0, 13,  3, 14, 60, 13, 14, 60, 44, 61],\n",
      "       [ 6, 17,  0,  2,  8,  0, 18, 18, 60, 51, 55, 61],\n",
      "       [ 9,  0,  9,  0,  9,  0,  9,  0,  9,  9,  0, 61],\n",
      "       [ 5,  0, 11, 19,  0,  1,  0, 18, 60, 19, 20, 61],\n",
      "       [13,  8, 60,  8,  3,  4,  0, 60, 46, 60, 51, 61],\n",
      "       [ 3, 14, 22, 13, 60,  9,  0,  9,  0,  9,  0, 61],\n",
      "       [14, 13, 19,  0,  0,  0, 60,  9,  0,  9,  0, 61],\n",
      "       [19,  0, 10,  4, 60,  0, 60,  7,  8, 13, 19, 61]])]\n"
     ]
    }
   ],
   "source": [
    "print(next(gb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T11:46:07.908359Z",
     "start_time": "2021-05-08T11:46:07.898109Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-714-24cd5a6a9649>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpad_tweets_noised_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_tweets_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_tweets_noised_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_tweets_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 2)"
     ]
    }
   ],
   "source": [
    "pad_tweets_noised_batch, pad_tweets_batch, pad_tweets_noised_lengths, pad_tweets_lengths = next(gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T11:46:09.597134Z",
     "start_time": "2021-05-08T11:46:09.594117Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8 60  0  3 14 17  4 60 24 14 20 61]\n",
      " [14  7 60 14 10  0 24 60 47 60 47 61]\n",
      " [22 14  0  7  7  7  7  7 60  2 23 61]\n",
      " [ 8 19 18 60 17  4  3 60 11 14 11 61]\n",
      " [ 7  0  7  0  7  7  0  7  0  7  2 61]\n",
      " [14  7 60 14  7 60 14  7 60 14  7 61]\n",
      " [14 13 19  0 60 11  0 60 44 44 44 61]\n",
      " [10  4 12  0 11 14 18 18 18 18 18 61]]\n"
     ]
    }
   ],
   "source": [
    "print(pad_tweets_noised_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T11:46:09.988542Z",
     "start_time": "2021-05-08T11:46:09.985009Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8 60  0  3 14 17  4 60 24 14 20 61]\n",
      " [14  7 60 14 10  0 24 60 47 60 47 61]\n",
      " [22 14  0  7  7  7  7  7 60  2 23 61]\n",
      " [ 8 19 18 60 17  4  3 60 11 14 11 61]\n",
      " [ 7  0  7  0  7  7  0  7  0  7  0 61]\n",
      " [14  7 60 14  7 60 14  7 60 14  7 61]\n",
      " [14 13 19  0 60 11  0 60 44 44 44 61]\n",
      " [10  4 12  0 11 14 18 18 18 18 18 61]]\n"
     ]
    }
   ],
   "source": [
    "print(pad_tweets_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T11:46:10.901307Z",
     "start_time": "2021-05-08T11:46:10.898031Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.2\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout, LSTM, RepeatVector\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T11:46:12.545685Z",
     "start_time": "2021-05-08T11:46:11.529457Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "DROPOUT = RECURRENT_DROPOUT = 0.8\n",
    "HIDDEN_SIZE = 50\n",
    "\n",
    "model.add(LSTM(units=HIDDEN_SIZE, input_shape=(None, len(VOCAB_TO_INT)),\n",
    "               kernel_initializer=\"he_normal\", dropout=DROPOUT, recurrent_dropout=RECURRENT_DROPOUT))\n",
    "\n",
    "output_len = 100\n",
    "\n",
    "model.add(RepeatVector(output_len))\n",
    "\n",
    "LAYERS = 2\n",
    "\n",
    "for _ in range(LAYERS):\n",
    "    model.add(LSTM(HIDDEN_SIZE, return_sequences=True,\n",
    "                   kernel_initializer=\"he_normal\", dropout=DROPOUT, recurrent_dropout=RECURRENT_DROPOUT))\n",
    "    \n",
    "model.add(Dense(len(VOCAB_TO_INT), kernel_initializer=\"he_normal\"))\n",
    "\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T13:58:27.732658Z",
     "start_time": "2021-05-08T13:58:27.719367Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Error when checking model input: data should be a Numpy array, or list/dict of Numpy arrays. Found: <generator object get_batches at 0x7fa700ea66d0>...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-720-defb3179c69f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_sorted_medium_lines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoiser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_sorted_medium_lines\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/capstone_project/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    958\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m~/opt/anaconda3/envs/capstone_project/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1579\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1580\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1581\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1582\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1583\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/capstone_project/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1412\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1413\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1414\u001b[0;31m                                     exception_prefix='input')\n\u001b[0m\u001b[1;32m   1415\u001b[0m         y = _standardize_input_data(y, self._feed_output_names,\n\u001b[1;32m   1416\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/capstone_project/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    111\u001b[0m                             \u001b[0;34m': data should be a Numpy array, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                             \u001b[0;34m'or list/dict of Numpy arrays. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                             'Found: ' + str(data)[:200] + '...')\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;31m# Case: model expects multiple inputs but only received\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Error when checking model input: data should be a Numpy array, or list/dict of Numpy arrays. Found: <generator object get_batches at 0x7fa700ea66d0>..."
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "batches = get_batches(clean_sorted_medium_lines, noiser, batch_size=BATCH_SIZE)\n",
    "\n",
    "model.fit(batches, steps_per_epoch=len(clean_sorted_medium_lines) // BATCH_SIZE, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
