{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T11:52:20.889935Z",
     "start_time": "2021-06-03T11:52:18.463260Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T11:52:20.895719Z",
     "start_time": "2021-06-03T11:52:20.892230Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.18.5 2.5.0 2.5.0\n"
     ]
    }
   ],
   "source": [
    "print(np.__version__, tf.__version__, keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T11:52:20.905012Z",
     "start_time": "2021-06-03T11:52:20.899535Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_tweet(tweet: str) -> str:\n",
    "    # take out URLs\n",
    "    tweet = re.sub(\"http.*(\\s|$)\", ' ', tweet)\n",
    "    # take out mentions\n",
    "    tweet = re.sub(\"@[^\\s]+\", ' ', tweet)\n",
    "    # take out hashtags\n",
    "    tweet = re.sub(\"#[^\\s]+\", ' ', tweet)\n",
    "    # take out all characters outside of those we enumerate\n",
    "    tweet = re.sub(\"[^\\da-zA-Z√°√©√≠√≥√∫√º√±√Å√â√ç√ë√ì√ö√ú¬ø?¬°!.,;#:<>()'‚Äú‚Äù\\\"\\s]\", ' ', tweet)\n",
    "    # lowercase everything\n",
    "    tweet = tweet.lower()\n",
    "    # reset spaces\n",
    "    tweet = re.sub(\"\\s+\", ' ', tweet)\n",
    "    tweet = re.sub(\"^\\s+\", '', tweet)\n",
    "    tweet = re.sub(\"\\s$\", '', tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T11:52:20.925466Z",
     "start_time": "2021-06-03T11:52:20.907000Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to go to san Antonio !!! and to hill country and san marcos too\n",
      "i want to go to san antonio !!! and to hill country and san marcos too\n",
      "@JimmyEsqueda YYAAAAAAAAAAAAAAAAAAAAAAAAAS ! üòÅ but hes gonna bite you üòîüêæ\n",
      "yyaaaaaaaaaaaaaaaaaaaaaaaaas ! but hes gonna bite you\n",
      "hoy es el ultimo sleepless sleepover:')\n",
      "hoy es el ultimo sleepless sleepover:')\n",
      "I feel horrible :(\n",
      "i feel horrible :(\n",
      "@adrianaaleee can u shut up ?\n",
      "can u shut up ?\n",
      "JAJAJAJAJAJAJA BYEEEE https://t.co/VnfjEiyasc\n",
      "jajajajajajaja byeeee\n",
      "\" Houston hasta MX .. compralo vendelo SPM dejalo .. Y fierro en este juego necesitas huevos ! \"\n",
      "\" houston hasta mx .. compralo vendelo spm dejalo .. y fierro en este juego necesitas huevos ! \"\n",
      "Sad but true üíÅ\n",
      "sad but true\n",
      "üòí\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# spotchecking loop to check that clean_tweet() works\n",
    "with open('lid_train_lines.txt', 'r') as input_file:\n",
    "    for line in input_file:\n",
    "        if random.random() < 0.0005:\n",
    "            line = line.strip()\n",
    "            print(line)\n",
    "            print(clean_tweet(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T11:52:21.241626Z",
     "start_time": "2021-06-03T11:52:20.926988Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_lines = []\n",
    "\n",
    "with open('lid_train_lines.txt', 'r') as input_file:\n",
    "    for line in input_file:\n",
    "        clean_lines.append(clean_tweet(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T11:52:21.252217Z",
     "start_time": "2021-06-03T11:52:21.243716Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21030"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T11:52:21.256641Z",
     "start_time": "2021-06-03T11:52:21.254091Z"
    }
   },
   "outputs": [],
   "source": [
    "random.seed(a=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T11:52:21.263703Z",
     "start_time": "2021-06-03T11:52:21.261039Z"
    }
   },
   "outputs": [],
   "source": [
    "chosen_tweets = random.choices(clean_lines, k=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T11:52:21.271632Z",
     "start_time": "2021-06-03T11:52:21.266415Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ma√±ana viernes wilfrido vargas ..',\n",
       " 'u love me',\n",
       " 'so original .. .',\n",
       " 'whats up with nature',\n",
       " 'aguanta tu ritmo .',\n",
       " \"i do n't like ( xxx ) people , they always think they 're right and do n't listen to anything else . ( testarudo a obstinado a tercho a )\",\n",
       " 'ni pinches moscas pescas tu mondaooo',\n",
       " 'new week , be kind and sweet ! vestido sahara',\n",
       " 'no puedo bregar con la gente stalker',\n",
       " 'a cantarle las ma√±anitas ala virgencita',\n",
       " 'el vlog de hoy aunque sea tarde , peque√±as cenas agradables : el d√≠a m√°s feliz de mi vida',\n",
       " 'all bad',\n",
       " 'no es sin√≥nimo pero significa lo mismo , i get out of work at 6 pm , creo q esto es lo q quer√≠as decir :)',\n",
       " 'blusa violeta y sus diferentes estampados ! escoge tu color favorito made in ecuador',\n",
       " '¬°¬° nuevo v√≠deo !! en esta ocasi√≥n , un famoso presentador de tv me hace una dur√≠sima entrevista .. . ¬° se .. .',\n",
       " 'get the hang of it : acostumbrarse a esto poder dominarlo person 1 : this new job is so stressful . person 2 : you ll get the hang of it .',\n",
       " 'clase de text',\n",
       " 'voy a empezar a pedirle trabajos a la gente como me los piden a mi',\n",
       " 'en estos momentos prefiero hacer mi ensayo de psicolog√≠a que trabajar',\n",
       " 'online class de sexual harassment .. . arianna is so mean',\n",
       " '\" y nunca me entretengo aber si me aman , les doy mi corazon por una semana , y luego dejo k se alejen de mi si les da la gana \"',\n",
       " \"ruth likes my dad 's new truck\",\n",
       " '‚Äú : si el camino es dif√≠cil .. . normalmente es porque vas en la direcci√≥n correcta . ‚Äù very true !! !',\n",
       " 'been waiting haha',\n",
       " 'lol jk but gracias corazon',\n",
       " '\" : my best friend',\n",
       " 'gracias flaca y el vestido es de la marca .',\n",
       " 'haber saludao !',\n",
       " 'u love me',\n",
       " 'unless they cut my hours again and tell me bout to go in today',\n",
       " 'es fun ver lo que dicen y eso jaja .',\n",
       " 'this bop put bacon on my food omfg foh',\n",
       " '',\n",
       " 'yo no pregunte pq a la defensiva ?',\n",
       " 'comenz√≥ la recta final a 1 mes y 1 d√≠a !! ! omg : o',\n",
       " 'la respuesta de es :',\n",
       " 'tu sabes que no',\n",
       " 'a escucharlo y disfrutarlo ! ( si te gusta claro )',\n",
       " 'te relacionas ? jajaja',\n",
       " '‚Äú : i miss my favorite stylist ! ‚Äù miss u to chica',\n",
       " 'ay y yo , pero tengo el estomago vacio y me iria over',\n",
       " 'si quieren un poco de inspiracion busquen el poema de steve jobs \" here \\'s to the crazy ones \"',\n",
       " 'fuhk valley is gunna close already',\n",
       " 'so si te van aser ?',\n",
       " 'a la verdad que el verano despues de freshmen year yo estaba obesa',\n",
       " \"plain and simple . simple y llanamente . i do n t need a new car , i 'm not interested , plain and simple\",\n",
       " 'as√≠ o m√°s energ√≠a ? la mayor cantidad de m√∫sica',\n",
       " 'that was so emberrasing',\n",
       " 'lol your all bad',\n",
       " 'tfti !! i want chokomil lmao',\n",
       " 'mi perrita abda de latosa y no me deja dormir .',\n",
       " 'me estan comprando una mac',\n",
       " 'a tus ni√±as les gusts el mundo del modelaje ? registralas en imagen modeling by la',\n",
       " \"yessss !! it does :d nd it 's gonna help us muajajaja . ;)\",\n",
       " 'gastar toel dinero que hiciste ?',\n",
       " 'buen provecho !',\n",
       " 'todos ayer jajaja',\n",
       " 'me metieron con un guineo en la cara ! jajajajjaj',\n",
       " 'quisiera volver a ser peque√±o , no pagar renta , ni usar desodrante .',\n",
       " '‚Äú : que guapa esperemos pronto verte por mexico saludos de todo el club de hp ‚Äù gracias chica . saludos a todos',\n",
       " 'go with the flow : varias cosas una sigue a la mayor√≠a p1 : are we going to plan the trip ? p2 : let s just go with the flow and see what happen',\n",
       " \"jajajajja ayy precious have you tried the chocolate filled gansitos ?? they 're sooooo good !!!\",\n",
       " 'la he escuchado mucho , pero no la usaba :)',\n",
       " 'can you ? lol',\n",
       " 'tengo el brazo como el muslo de beyonc√© , pero totally worth it . buenos d√≠as .',\n",
       " 'hate being so detailed when describing a person because i hit their qualities spot on when they describe me they make me sound so blah .',\n",
       " 'really what is it about ? i wanted too go see it lmao',\n",
       " 'all my tios are coming to my house .... like always',\n",
       " 'jajajajaja mira ponte a estudiar para el depa de ma√±ana y shh',\n",
       " 'life long dream ? ir a un concierto de coldplay',\n",
       " 'beffas beers pool',\n",
       " 'imagen modeling by la gatita reg√≠strate en',\n",
       " 'la primera es la correcta , no me expliqu√© bien :( can you tell me who she is , can you tell me where it is , do you know who he is :)',\n",
       " 'no',\n",
       " 'hay alguien que o no hablo en todo el dia o tiene la mente a millon',\n",
       " 'down like a real friends supposed to im trying to show you the life of somebody like you should be living',\n",
       " 'gracias',\n",
       " \"oh no ! let 's figure this out , edgar . could you please follow us and send us a dm for further assistance ? thanks ! mh\",\n",
       " 'gracias mi ponchin !',\n",
       " 'make them your strength',\n",
       " 'con cari√±o , una canci√≥n , . con√©ctate a la fiesta',\n",
       " '‚Äú : gracias via x mi para √±a2014 ‚Äù',\n",
       " 'jajajaja yo',\n",
       " 'my brothers are honestly amazing !!! i loved every single one of the presents they gave me !',\n",
       " 'no lmfao i was just not sleepy jajaja .',\n",
       " 'lo mejor hasta ahorita son y mi bella',\n",
       " '\" : alcoholic ? no , i prefer the term drinking enthusiast . \" jajajajaja accurate ?',\n",
       " 'me maltrato con un guineo .',\n",
       " 'nuestras faldas lucianas en sus diferentes colores para cada personalidad cu√°l prefieres ?',\n",
       " 'obsesionada con este personaje',\n",
       " '\" que mucha mierda has comido con la monografia esa .. \" pues hasla tu que tal ?',\n",
       " 'belvita !',\n",
       " 'no dejes que el lunes te ponga patas arriba ! empieza la semana con √°nimo . mi',\n",
       " 'para una noticiero estelar es necesario un outfit ! lucete como nuestra querida',\n",
       " 'los amo . ; un besote a mi por confiar en mi y a ustedes tambien .',\n",
       " \"lol okay it 's iaavalos\",\n",
       " \"omg this sucks !! i hate being sick can n't even breath :(\",\n",
       " '‚Äú : to err is human , to forgive , devine . ‚Äù',\n",
       " 'exacto',\n",
       " 'your not going to find a girl who loves you more than me']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chosen_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T11:52:21.277903Z",
     "start_time": "2021-06-03T11:52:21.273209Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ma√±ana viernes wilfrido vargas ..\n",
      "u love me\n",
      "so original .. .\n",
      "whats up with nature\n",
      "aguanta tu ritmo .\n",
      "i do n't like ( xxx ) people , they always think they 're right and do n't listen to anything else . ( testarudo a obstinado a tercho a )\n",
      "ni pinches moscas pescas tu mondaooo\n",
      "new week , be kind and sweet ! vestido sahara\n",
      "no puedo bregar con la gente stalker\n",
      "a cantarle las ma√±anitas ala virgencita\n",
      "el vlog de hoy aunque sea tarde , peque√±as cenas agradables : el d√≠a m√°s feliz de mi vida\n",
      "all bad\n",
      "no es sin√≥nimo pero significa lo mismo , i get out of work at 6 pm , creo q esto es lo q quer√≠as decir :)\n",
      "blusa violeta y sus diferentes estampados ! escoge tu color favorito made in ecuador\n",
      "¬°¬° nuevo v√≠deo !! en esta ocasi√≥n , un famoso presentador de tv me hace una dur√≠sima entrevista .. . ¬° se .. .\n",
      "get the hang of it : acostumbrarse a esto poder dominarlo person 1 : this new job is so stressful . person 2 : you ll get the hang of it .\n",
      "clase de text\n",
      "voy a empezar a pedirle trabajos a la gente como me los piden a mi\n",
      "en estos momentos prefiero hacer mi ensayo de psicolog√≠a que trabajar\n",
      "online class de sexual harassment .. . arianna is so mean\n",
      "\" y nunca me entretengo aber si me aman , les doy mi corazon por una semana , y luego dejo k se alejen de mi si les da la gana \"\n",
      "ruth likes my dad 's new truck\n",
      "‚Äú : si el camino es dif√≠cil .. . normalmente es porque vas en la direcci√≥n correcta . ‚Äù very true !! !\n",
      "been waiting haha\n",
      "lol jk but gracias corazon\n",
      "\" : my best friend\n",
      "gracias flaca y el vestido es de la marca .\n",
      "haber saludao !\n",
      "u love me\n",
      "unless they cut my hours again and tell me bout to go in today\n",
      "es fun ver lo que dicen y eso jaja .\n",
      "this bop put bacon on my food omfg foh\n",
      "\n",
      "yo no pregunte pq a la defensiva ?\n",
      "comenz√≥ la recta final a 1 mes y 1 d√≠a !! ! omg : o\n",
      "la respuesta de es :\n",
      "tu sabes que no\n",
      "a escucharlo y disfrutarlo ! ( si te gusta claro )\n",
      "te relacionas ? jajaja\n",
      "‚Äú : i miss my favorite stylist ! ‚Äù miss u to chica\n",
      "ay y yo , pero tengo el estomago vacio y me iria over\n",
      "si quieren un poco de inspiracion busquen el poema de steve jobs \" here 's to the crazy ones \"\n",
      "fuhk valley is gunna close already\n",
      "so si te van aser ?\n",
      "a la verdad que el verano despues de freshmen year yo estaba obesa\n",
      "plain and simple . simple y llanamente . i do n t need a new car , i 'm not interested , plain and simple\n",
      "as√≠ o m√°s energ√≠a ? la mayor cantidad de m√∫sica\n",
      "that was so emberrasing\n",
      "lol your all bad\n",
      "tfti !! i want chokomil lmao\n",
      "mi perrita abda de latosa y no me deja dormir .\n",
      "me estan comprando una mac\n",
      "a tus ni√±as les gusts el mundo del modelaje ? registralas en imagen modeling by la\n",
      "yessss !! it does :d nd it 's gonna help us muajajaja . ;)\n",
      "gastar toel dinero que hiciste ?\n",
      "buen provecho !\n",
      "todos ayer jajaja\n",
      "me metieron con un guineo en la cara ! jajajajjaj\n",
      "quisiera volver a ser peque√±o , no pagar renta , ni usar desodrante .\n",
      "‚Äú : que guapa esperemos pronto verte por mexico saludos de todo el club de hp ‚Äù gracias chica . saludos a todos\n",
      "go with the flow : varias cosas una sigue a la mayor√≠a p1 : are we going to plan the trip ? p2 : let s just go with the flow and see what happen\n",
      "jajajajja ayy precious have you tried the chocolate filled gansitos ?? they 're sooooo good !!!\n",
      "la he escuchado mucho , pero no la usaba :)\n",
      "can you ? lol\n",
      "tengo el brazo como el muslo de beyonc√© , pero totally worth it . buenos d√≠as .\n",
      "hate being so detailed when describing a person because i hit their qualities spot on when they describe me they make me sound so blah .\n",
      "really what is it about ? i wanted too go see it lmao\n",
      "all my tios are coming to my house .... like always\n",
      "jajajajaja mira ponte a estudiar para el depa de ma√±ana y shh\n",
      "life long dream ? ir a un concierto de coldplay\n",
      "beffas beers pool\n",
      "imagen modeling by la gatita reg√≠strate en\n",
      "la primera es la correcta , no me expliqu√© bien :( can you tell me who she is , can you tell me where it is , do you know who he is :)\n",
      "no\n",
      "hay alguien que o no hablo en todo el dia o tiene la mente a millon\n",
      "down like a real friends supposed to im trying to show you the life of somebody like you should be living\n",
      "gracias\n",
      "oh no ! let 's figure this out , edgar . could you please follow us and send us a dm for further assistance ? thanks ! mh\n",
      "gracias mi ponchin !\n",
      "make them your strength\n",
      "con cari√±o , una canci√≥n , . con√©ctate a la fiesta\n",
      "‚Äú : gracias via x mi para √±a2014 ‚Äù\n",
      "jajajaja yo\n",
      "my brothers are honestly amazing !!! i loved every single one of the presents they gave me !\n",
      "no lmfao i was just not sleepy jajaja .\n",
      "lo mejor hasta ahorita son y mi bella\n",
      "\" : alcoholic ? no , i prefer the term drinking enthusiast . \" jajajajaja accurate ?\n",
      "me maltrato con un guineo .\n",
      "nuestras faldas lucianas en sus diferentes colores para cada personalidad cu√°l prefieres ?\n",
      "obsesionada con este personaje\n",
      "\" que mucha mierda has comido con la monografia esa .. \" pues hasla tu que tal ?\n",
      "belvita !\n",
      "no dejes que el lunes te ponga patas arriba ! empieza la semana con √°nimo . mi\n",
      "para una noticiero estelar es necesario un outfit ! lucete como nuestra querida\n",
      "los amo . ; un besote a mi por confiar en mi y a ustedes tambien .\n",
      "lol okay it 's iaavalos\n",
      "omg this sucks !! i hate being sick can n't even breath :(\n",
      "‚Äú : to err is human , to forgive , devine . ‚Äù\n",
      "exacto\n",
      "your not going to find a girl who loves you more than me\n"
     ]
    }
   ],
   "source": [
    "for chosen_tweet in chosen_tweets:\n",
    "    print (chosen_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T11:52:21.282956Z",
     "start_time": "2021-06-03T11:52:21.279382Z"
    }
   },
   "outputs": [],
   "source": [
    "DISACCENT_MAP = {'√°': 'a', '√©': 'e', '√≠': 'i', '√≥': 'o', '√∫': 'u', '√º': 'u', '√±': 'n'}\n",
    "\n",
    "def drop_accents(tweet: [str], drop_probability: float=1.0) -> [str]:\n",
    "    output = []\n",
    "    for c in tweet:\n",
    "        if c in DISACCENT_MAP and random.random() < drop_probability:\n",
    "            output.append(DISACCENT_MAP[c])\n",
    "        else:\n",
    "            output.append(c)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T11:52:21.288841Z",
     "start_time": "2021-06-03T11:52:21.285258Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jajajaajaja yo odio la palabra \" mocion \" con mi vida entera despues de la estupidez de huelga esa'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = 'jajajaajaja yo odio la palabra \" moci√≥n \" con mi vida entera despu√©s de la estupidez de huelga esa'\n",
    "''.join(drop_accents(example, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T11:52:21.293847Z",
     "start_time": "2021-06-03T11:52:21.290516Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21030"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T11:52:21.299519Z",
     "start_time": "2021-06-03T11:52:21.295551Z"
    }
   },
   "outputs": [],
   "source": [
    "VOWELS_SET = set(\"aeiou√°√©√≠√≥√∫√º\")\n",
    "\n",
    "def drop_vowels(tweet: [str], drop_probability: float=1.0) -> [str]:\n",
    "    output = []\n",
    "    for c in tweet:\n",
    "        if c in VOWELS_SET and random.random() < drop_probability:\n",
    "            continue\n",
    "        output.append(c)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T11:52:21.307472Z",
     "start_time": "2021-06-03T11:52:21.302790Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jajajajaja yo odo la palabr \" moci√≥n \" con m vida entera despu√©s de la estpidez de huelga esa'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(drop_vowels(example, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T11:52:21.313725Z",
     "start_time": "2021-06-03T11:52:21.309473Z"
    }
   },
   "outputs": [],
   "source": [
    "def repeat_vowels(tweet: [str], repeat_probability=0.05, max_repeat=6) -> [str]:\n",
    "    output = []\n",
    "    for c in tweet:\n",
    "        if c in VOWELS_SET and random.random() < repeat_probability:\n",
    "            for _ in range(random.randint(1, max_repeat)):\n",
    "                output.append(c)\n",
    "        output.append(c)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T11:52:21.320439Z",
     "start_time": "2021-06-03T11:52:21.315623Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jajajaajaaaaaaja yo odio la palabra \" moci√≥n \" con miiii vida entera despu√©s de la estupidez dee huelga esa'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(repeat_vowels(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T11:52:21.325749Z",
     "start_time": "2021-06-03T11:52:21.322224Z"
    }
   },
   "outputs": [],
   "source": [
    "CREATIVE_SUB_MAP = {'c': 's', 's': 'z', 'b': 'v', 'v': 'b'}\n",
    "\n",
    "def substitute_creatively(tweet: [str], substitution_probability=0.1):\n",
    "    output = []\n",
    "    for c in tweet:\n",
    "        if c in CREATIVE_SUB_MAP and random.random() < substitution_probability:\n",
    "            output.append(CREATIVE_SUB_MAP[c])\n",
    "        else:\n",
    "            output.append(c)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T11:52:21.331056Z",
     "start_time": "2021-06-03T11:52:21.327568Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jajajaajaja yo odio la palabra \" moci√≥n \" con mi vida entera despu√©s de la estupidez de huelga eza'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(substitute_creatively(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T11:52:21.335898Z",
     "start_time": "2021-06-03T11:52:21.332898Z"
    }
   },
   "outputs": [],
   "source": [
    "def simulate_creative_misspellings(tweet: [str]) -> [str]:\n",
    "    if random.random() < 0.7:\n",
    "        return repeat_vowels(tweet)\n",
    "    else:\n",
    "        return substitute_creatively(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T11:52:21.341306Z",
     "start_time": "2021-06-03T11:52:21.337714Z"
    }
   },
   "outputs": [],
   "source": [
    "def simulate_intentional_shortenings(tweet: [str]) -> [str]:\n",
    "    input_tokens = ''.join(tweet).split(' ')\n",
    "    output_tokens = []\n",
    "    for token in input_tokens:\n",
    "        if random.random() < 0.3:\n",
    "            output_tokens.append(''.join(drop_vowels(list(token), 0.5)))\n",
    "        else:\n",
    "            output_tokens.append(token)\n",
    "    return list(' '.join(output_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T11:52:21.346678Z",
     "start_time": "2021-06-03T11:52:21.343230Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jajajaajaja yo odio l palabra \" moci√≥n \" con mi vida entera despus d la estupidez de huelga esa'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(simulate_intentional_shortenings(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T11:52:21.360112Z",
     "start_time": "2021-06-03T11:52:21.356558Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '√°', '√©', '√≠', '√≥', '√∫', '√º', '√±']\n",
      "{'a', 'z', '√º', '√©', 'o', 'i', 'h', 's', 'r', 'l', 'v', 'w', '√≠', 'q', 'd', 'j', '√∫', '√±', 'm', 'x', 'f', '√≥', 'g', 'b', 'c', 'k', 'p', 'u', 't', 'n', '√°', 'y', 'e'}\n"
     ]
    }
   ],
   "source": [
    "ALPHABET_L = list(\"abcdefghijklmnopqrstuvwxyz√°√©√≠√≥√∫√º√±\")\n",
    "ALPHABET_S = set(ALPHABET_L)\n",
    "print(ALPHABET_L)\n",
    "print(ALPHABET_S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T11:52:21.369165Z",
     "start_time": "2021-06-03T11:52:21.365019Z"
    }
   },
   "outputs": [],
   "source": [
    "def simulate_bona_fide_spelling_mistakes(tweet: [str], modify_rate=0.1) -> [str]:\n",
    "    # equally add, omit, or substitute each character for a total probability of modify_rate\n",
    "    add_cmf_val = modify_rate / 3\n",
    "    omit_cmf_val = 2 * modify_rate / 3\n",
    "    substitute_cmf_val = modify_rate\n",
    "    \n",
    "    output = []\n",
    "    for c in tweet:\n",
    "        r = random.random()\n",
    "        if r < add_cmf_val:\n",
    "            output.append(c)\n",
    "            output.append(random.choice(ALPHABET_L))\n",
    "        elif r < omit_cmf_val:\n",
    "            continue\n",
    "        elif c in ALPHABET_S and r < substitute_cmf_val:\n",
    "            # we don't apply substitutions to non-alpha\n",
    "            output.append(random.choice(ALPHABET_L))\n",
    "        else:\n",
    "            output.append(c)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T11:52:21.374858Z",
     "start_time": "2021-06-03T11:52:21.371366Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jajajaajaja vo odio la palabra \" moci√≥n \" con mi vida etea despu√©s dte lx esteupidez de huega esa'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(simulate_bona_fide_spelling_mistakes(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T11:52:21.378912Z",
     "start_time": "2021-06-03T11:52:21.376512Z"
    }
   },
   "outputs": [],
   "source": [
    "def identity(tweet: [str]) -> [str]:\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T11:52:21.386642Z",
     "start_time": "2021-06-03T11:52:21.380715Z"
    }
   },
   "outputs": [],
   "source": [
    "DEFAULT_WEIGHT_MAP = [[14, simulate_creative_misspellings],\n",
    "                      [13, drop_accents],\n",
    "                      [10, simulate_intentional_shortenings],\n",
    "                      [8, simulate_bona_fide_spelling_mistakes],\n",
    "                      [10, identity]]\n",
    "\n",
    "class DisjointNoiser:\n",
    "    def __init__(self, weight_map=DEFAULT_WEIGHT_MAP):\n",
    "        self.weight_map = weight_map\n",
    "        total = sum([self.weight_map[i][0] for i in range(len(self.weight_map))])\n",
    "        # we normalize the pmf\n",
    "        for i in range(len(self.weight_map)):\n",
    "            self.weight_map[i][0] /= total\n",
    "        # we make the pmf into a cmf\n",
    "        for i in range(1, len(self.weight_map) - 1):\n",
    "            self.weight_map[i][0] += self.weight_map[i - 1][0]\n",
    "        self.weight_map[-1][0] = 1.0  # so as not to worry about rounding errors\n",
    "\n",
    "    def add_noise(self, tweet: [str]) -> [str]:\n",
    "        \"\"\"Applies at most 1 kind of noising to the tweet according to the weights in the weight map.\n",
    "        Each 'noising' could alter the tweet in multiple places or not at all.\n",
    "        \"\"\"\n",
    "        tweet = list(tweet)\n",
    "        my_random_number = random.random()\n",
    "        for max_prob, noise_function in self.weight_map:\n",
    "            if my_random_number < max_prob:\n",
    "                # print('calling', noise_function)\n",
    "                return noise_function(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T11:52:21.391228Z",
     "start_time": "2021-06-03T11:52:21.388491Z"
    }
   },
   "outputs": [],
   "source": [
    "noiser = DisjointNoiser()\n",
    "for _ in range(10):\n",
    "    noiser.add_noise('abcd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T11:52:21.399104Z",
     "start_time": "2021-06-03T11:52:21.393810Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jjajjaja yo odio la palabra \" moci√≥n \" con mi vida entera despu√©s de la estupidez de huelga esa\n",
      "\n",
      "jajajaajaja yo odio l palabra \" moci√≥n \" con m vida entera dspu√©s de la stpidz de huelga esa\n",
      "\n",
      "jajajaajaja yo odio la palabra \" moci√≥n \" con mi vida entera despu√©s de la estupidez de huelga esa\n",
      "\n",
      "jajajaajaja yo odio la palabra \" mocion \" con mi vida entera despues de la estupidez de huelga esa\n",
      "\n",
      "jajajaajaja yo odio la palabra \" moci√≥n \" con mi vida entera despu√©s de la estupidez de huelga esa\n",
      "\n",
      "jajajaajaja yo odio la palabra \" moci√≥n \" con mi vida entera despu√©s de la estupidez de huelga esa\n",
      "\n",
      "jajajajfja yo odio la ypalabra \" oci√≥n \" con mi vviada etera despu√©s de la estufpidez de huelga qsa\n",
      "\n",
      "jajajaajaja yo odio la palabra \" mci√≥n \" con mi vd entera desps de la estupidez de hulga esa\n",
      "\n",
      "jjajaajajay yo di√° la alabra \" moci√≥n \" con mi vida entera despu√©s de la estoupidez de huelga es\n",
      "\n",
      "jajajaajavja y odiolj plabra \" moci√≥n c\" csn mi visa entera d√±espu√© de la estupidez de helga xsa\n",
      "\n"
     ]
    }
   ],
   "source": [
    "noiser = DisjointNoiser()\n",
    "for _ in range(10):\n",
    "    print(''.join(noiser.add_noise(example)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T11:52:21.405846Z",
     "start_time": "2021-06-03T11:52:21.401778Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21030"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T11:52:21.413044Z",
     "start_time": "2021-06-03T11:52:21.407424Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(len(line) for line in clean_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T11:52:21.421094Z",
     "start_time": "2021-06-03T11:52:21.414684Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49.6810746552544"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(len(line) for line in clean_lines) / len(clean_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T11:52:21.428280Z",
     "start_time": "2021-06-03T11:52:21.423267Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_medium_lines = [line for line in clean_lines if 10 < len(line) < 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T11:52:21.433762Z",
     "start_time": "2021-06-03T11:52:21.430041Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16908"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_medium_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T11:52:21.439940Z",
     "start_time": "2021-06-03T11:52:21.436093Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '√°', '√©', '√≠', '√≥', '√∫', '√º', '√±', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '¬ø', '?', '¬°', '!', '.', ',', ';', '#', ':', '<', '>', '(', ')', \"'\", '‚Äú', '‚Äù', '\"', ' ', '<GO>', '<EOT>', '<PAD>']\n"
     ]
    }
   ],
   "source": [
    "VOCAB_L = ALPHABET_L + list(\"0123456789¬ø?¬°!.,;#:<>()'‚Äú‚Äù\\\" \") + ['<GO>', '<EOT>', '<PAD>']\n",
    "print(VOCAB_L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T11:52:21.446036Z",
     "start_time": "2021-06-03T11:52:21.442657Z"
    }
   },
   "outputs": [],
   "source": [
    "VOCAB_TO_INT = {v: i for i, v in enumerate(VOCAB_L)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T11:52:21.451637Z",
     "start_time": "2021-06-03T11:52:21.448457Z"
    }
   },
   "outputs": [],
   "source": [
    "NUM_TOKENS = len(VOCAB_TO_INT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T11:52:21.455775Z",
     "start_time": "2021-06-03T11:52:21.453490Z"
    }
   },
   "outputs": [],
   "source": [
    "example_ints = [VOCAB_TO_INT[c] for c in example]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T11:52:21.460579Z",
     "start_time": "2021-06-03T11:52:21.457735Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 0, 9, 0, 9, 0, 0, 9, 0, 9, 0, 60, 24, 14, 60, 14, 3, 8, 14, 60, 11, 0, 60, 15, 0, 11, 0, 1, 17, 0, 60, 59, 60, 12, 14, 2, 8, 29, 13, 60, 59, 60, 2, 14, 13, 60, 12, 8, 60, 21, 8, 3, 0, 60, 4, 13, 19, 4, 17, 0, 60, 3, 4, 18, 15, 20, 27, 18, 60, 3, 4, 60, 11, 0, 60, 4, 18, 19, 20, 15, 8, 3, 4, 25, 60, 3, 4, 60, 7, 20, 4, 11, 6, 0, 60, 4, 18, 0]\n"
     ]
    }
   ],
   "source": [
    "print(example_ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T11:52:21.464949Z",
     "start_time": "2021-06-03T11:52:21.462358Z"
    }
   },
   "outputs": [],
   "source": [
    "def pad_tweet_batch(tweet_batch, max_tweet_length_in_batch):\n",
    "    \"\"\"Pad tweets with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
    "    return [tweet + [VOCAB_TO_INT['<PAD>']] * (max_tweet_length_in_batch - len(tweet)) for tweet in tweet_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T11:52:21.470339Z",
     "start_time": "2021-06-03T11:52:21.466525Z"
    }
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(padded_batch):\n",
    "    encoded_data = np.zeros(\n",
    "        (len(padded_batch), len(padded_batch[0]), NUM_TOKENS), dtype=\"float32\"\n",
    "    )\n",
    "    \n",
    "    for i, padded_text in enumerate(padded_batch):\n",
    "        for t, int_value in enumerate(padded_text):\n",
    "            encoded_data[i, t, int_value] = 1.0\n",
    "    \n",
    "    return encoded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T11:52:21.480745Z",
     "start_time": "2021-06-03T11:52:21.472694Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_batches(tweets, noiser, batch_size):\n",
    "    \n",
    "    for batch_i in range(0, len(tweets) // batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        tweets_batch = tweets[start_i:start_i + batch_size]\n",
    "        tweets_batch_noised = [noiser.add_noise(tweet) for tweet in tweets_batch]\n",
    "        \n",
    "        tweets_batch_ints = [[VOCAB_TO_INT[v] for v in tweet] for tweet in tweets_batch]\n",
    "        tweets_batch_noised_ints = [[VOCAB_TO_INT[v] for v in tweet] for tweet in tweets_batch_noised]\n",
    "        \n",
    "        tweets_batch_eot = [tweet + [VOCAB_TO_INT['<EOT>']] for tweet in tweets_batch_ints]\n",
    "        tweets_batch_delayed_eot = [[VOCAB_TO_INT['<GO>']] + tweet + [VOCAB_TO_INT['<EOT>']] for tweet in tweets_batch_ints]\n",
    "        tweets_batch_noised_eot = [tweet + [VOCAB_TO_INT['<EOT>']] for tweet in tweets_batch_noised_ints]\n",
    "        \n",
    "        pad_tweets_batch = np.array(pad_tweet_batch(tweets_batch_eot,\n",
    "                                                    1 + max([len(tweet) for tweet in tweets_batch_eot])))\n",
    "        pad_tweets_delayed_batch = np.array(pad_tweet_batch(tweets_batch_eot,\n",
    "                                                    max([len(tweet) for tweet in tweets_batch_delayed_eot])))\n",
    "        pad_tweets_noised_batch = np.array(pad_tweet_batch(tweets_batch_noised_eot,\n",
    "                                                           max([len(tweet) for tweet in tweets_batch_noised_eot])))\n",
    "        \n",
    "        pad_tweets_encoded_batch = one_hot_encode(pad_tweets_batch)\n",
    "        pad_tweets_delayed_encoded_batch = one_hot_encode(pad_tweets_delayed_batch)\n",
    "        pad_tweets_noised_encoded_batch = one_hot_encode(pad_tweets_noised_batch)\n",
    "        \n",
    "        yield pad_tweets_noised_encoded_batch, pad_tweets_encoded_batch, pad_tweets_delayed_encoded_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T11:52:21.489141Z",
     "start_time": "2021-06-03T11:52:21.482619Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_sorted_medium_lines = sorted(clean_medium_lines, key=lambda t: len(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T11:52:21.493299Z",
     "start_time": "2021-06-03T11:52:21.490913Z"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "\n",
    "gb = get_batches(clean_sorted_medium_lines, noiser, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T11:52:21.511059Z",
     "start_time": "2021-06-03T11:52:21.495639Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[0., 0., 1., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [1., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 1., 0.]],\n",
       " \n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [1., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 1., 0.]],\n",
       " \n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [1., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [1., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 1., 0.]],\n",
       " \n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [1., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 1., 0.]]], dtype=float32),\n",
       " array([[[0., 0., 1., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [1., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 1., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 1.]],\n",
       " \n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [1., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 1., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 1.]],\n",
       " \n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [1., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [1., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 1., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 1.]],\n",
       " \n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [1., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 1., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 1.]]], dtype=float32),\n",
       " array([[[0., 0., 1., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [1., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 1., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 1.]],\n",
       " \n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [1., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 1., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 1.]],\n",
       " \n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [1., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [1., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 1., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 1.]],\n",
       " \n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [1., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 1., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 1.]]], dtype=float32))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T11:52:21.515168Z",
     "start_time": "2021-06-03T11:52:21.513041Z"
    }
   },
   "outputs": [],
   "source": [
    "# pad_tweets_noised_batch, pad_tweets_batch, pad_tweets_noised_lengths, pad_tweets_lengths = next(gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T11:52:21.520583Z",
     "start_time": "2021-06-03T11:52:21.517169Z"
    }
   },
   "outputs": [],
   "source": [
    "pad_tweets_noised_batch, pad_tweets_batch, pad_tweets_delayed_batch = next(gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T11:52:21.527158Z",
     "start_time": "2021-06-03T11:52:21.522519Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 1., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 1.],\n",
       "        [0., 0., 0., ..., 0., 0., 1.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 1., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 1., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 1.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 1., 0.]]], dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_tweets_noised_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T11:52:21.533731Z",
     "start_time": "2021-06-03T11:52:21.528845Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 1., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 1.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 1., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 1.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 1., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 1.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 1., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 1.]]], dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_tweets_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T11:52:21.540556Z",
     "start_time": "2021-06-03T11:52:21.535541Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 1., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 1.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 1., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 1.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 1., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 1.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 1., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 1.]]], dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_tweets_delayed_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T11:52:21.546917Z",
     "start_time": "2021-06-03T11:52:21.542563Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(VOCAB_TO_INT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T11:52:21.553257Z",
     "start_time": "2021-06-03T11:52:21.549206Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_TO_INT['a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T11:52:21.560769Z",
     "start_time": "2021-06-03T11:52:21.556178Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_TO_INT['b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T11:52:21.565694Z",
     "start_time": "2021-06-03T11:52:21.562939Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, Dropout, LSTM, RepeatVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T11:52:21.570315Z",
     "start_time": "2021-06-03T11:52:21.567428Z"
    }
   },
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "\n",
    "# DROPOUT = RECURRENT_DROPOUT = 0.3\n",
    "# HIDDEN_SIZE = 50\n",
    "\n",
    "# encoder_inputs = keras.Input(shape=(None, len(VOCAB_TO_INT)))\n",
    "#                kernel_initializer=\"he_normal\", dropout=DROPOUT, recurrent_dropout=RECURRENT_DROPOUT))\n",
    "\n",
    "# # output_len = 12\n",
    "\n",
    "# # model.add(RepeatVector(output_len))\n",
    "\n",
    "# LAYERS = 2\n",
    "\n",
    "# for _ in range(LAYERS):\n",
    "#     model.add(LSTM(HIDDEN_SIZE, return_sequences=True,\n",
    "#               kernel_initializer=\"he_normal\", dropout=DROPOUT, recurrent_dropout=RECURRENT_DROPOUT))\n",
    "\n",
    "# model.add(Dense(len(VOCAB_TO_INT), kernel_initializer=\"he_normal\"))\n",
    "\n",
    "# model.add(Activation('softmax'))\n",
    "\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T11:52:22.013826Z",
     "start_time": "2021-06-03T11:52:21.572758Z"
    }
   },
   "outputs": [],
   "source": [
    "latent_dim = 256\n",
    "\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = keras.Input(shape=(None, NUM_TOKENS))\n",
    "encoder = keras.layers.LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = keras.Input(shape=(None, NUM_TOKENS))\n",
    "\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = keras.layers.Dense(NUM_TOKENS, activation=\"softmax\")\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T11:52:22.022205Z",
     "start_time": "2021-06-03T11:52:22.015741Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T11:52:22.029651Z",
     "start_time": "2021-06-03T11:52:22.024001Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None, 64)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None, 64)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 256), (None, 328704      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 256),  328704      input_2[0][0]                    \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 64)     16448       lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 673,856\n",
      "Trainable params: 673,856\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T11:52:22.040505Z",
     "start_time": "2021-06-03T11:52:22.031495Z"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T11:52:22.044251Z",
     "start_time": "2021-06-03T11:52:22.042196Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T11:52:22.049318Z",
     "start_time": "2021-06-03T11:52:22.046091Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16908"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_sorted_medium_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T11:52:22.053647Z",
     "start_time": "2021-06-03T11:52:22.050879Z"
    }
   },
   "outputs": [],
   "source": [
    "batches_gen = get_batches(clean_sorted_medium_lines, noiser, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T11:52:22.057839Z",
     "start_time": "2021-06-03T11:52:22.055378Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T12:01:36.717084Z",
     "start_time": "2021-06-03T11:52:22.060115Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/d/opt/anaconda3/envs/py_3_7_tf_2_5/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py:3704: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable.debug_mode()`.\n",
      "  \"Even though the `tf.config.experimental_run_functions_eagerly` \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 111ms/step - loss: 3.4814 - accuracy: 0.1515\n",
      "0\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 2.9794 - accuracy: 0.2331\n",
      "1\n",
      "10/10 [==============================] - 1s 125ms/step - loss: 2.8601 - accuracy: 0.2386\n",
      "2\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 2.7259 - accuracy: 0.2707\n",
      "3\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 2.6583 - accuracy: 0.2827\n",
      "4\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 2.6793 - accuracy: 0.2860\n",
      "5\n",
      "10/10 [==============================] - 1s 127ms/step - loss: 2.4278 - accuracy: 0.3487\n",
      "6\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 2.4775 - accuracy: 0.3550\n",
      "7\n",
      "10/10 [==============================] - 1s 127ms/step - loss: 2.3169 - accuracy: 0.4250\n",
      "8\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 2.1756 - accuracy: 0.4594\n",
      "9\n",
      "10/10 [==============================] - 1s 128ms/step - loss: 2.0187 - accuracy: 0.5359\n",
      "10\n",
      "10/10 [==============================] - 1s 132ms/step - loss: 2.0173 - accuracy: 0.5912\n",
      "11\n",
      "10/10 [==============================] - 1s 131ms/step - loss: 1.7693 - accuracy: 0.6833\n",
      "12\n",
      "10/10 [==============================] - 1s 139ms/step - loss: 1.5449 - accuracy: 0.7539\n",
      "13\n",
      "10/10 [==============================] - 1s 131ms/step - loss: 1.4402 - accuracy: 0.7933\n",
      "14\n",
      "10/10 [==============================] - 1s 136ms/step - loss: 1.2789 - accuracy: 0.8558\n",
      "15\n",
      "10/10 [==============================] - 1s 145ms/step - loss: 1.0690 - accuracy: 0.9100\n",
      "16\n",
      "10/10 [==============================] - 1s 136ms/step - loss: 0.9075 - accuracy: 0.9242\n",
      "17\n",
      "10/10 [==============================] - 1s 148ms/step - loss: 0.7736 - accuracy: 0.9350\n",
      "18\n",
      "10/10 [==============================] - 2s 162ms/step - loss: 0.6773 - accuracy: 0.9575\n",
      "19\n",
      "10/10 [==============================] - 2s 154ms/step - loss: 0.5928 - accuracy: 0.9576\n",
      "20\n",
      "10/10 [==============================] - 1s 145ms/step - loss: 0.4675 - accuracy: 0.9686\n",
      "21\n",
      "10/10 [==============================] - 2s 157ms/step - loss: 0.4090 - accuracy: 0.9714\n",
      "22\n",
      "10/10 [==============================] - 2s 154ms/step - loss: 0.2715 - accuracy: 0.9818\n",
      "23\n",
      "10/10 [==============================] - 2s 159ms/step - loss: 0.3027 - accuracy: 0.9732\n",
      "24\n",
      "10/10 [==============================] - 2s 152ms/step - loss: 0.2355 - accuracy: 0.9791\n",
      "25\n",
      "10/10 [==============================] - 2s 167ms/step - loss: 0.2220 - accuracy: 0.9761\n",
      "26\n",
      "10/10 [==============================] - 2s 162ms/step - loss: 0.1368 - accuracy: 0.9904\n",
      "27\n",
      "10/10 [==============================] - 2s 167ms/step - loss: 0.1218 - accuracy: 0.9857\n",
      "28\n",
      "10/10 [==============================] - 2s 176ms/step - loss: 0.1109 - accuracy: 0.9858\n",
      "29\n",
      "10/10 [==============================] - 2s 165ms/step - loss: 0.0909 - accuracy: 0.9892\n",
      "30\n",
      "10/10 [==============================] - 2s 191ms/step - loss: 0.0780 - accuracy: 0.9912\n",
      "31\n",
      "10/10 [==============================] - 2s 179ms/step - loss: 0.0618 - accuracy: 0.9920\n",
      "32\n",
      "10/10 [==============================] - 2s 184ms/step - loss: 0.0758 - accuracy: 0.9900\n",
      "33\n",
      "10/10 [==============================] - 2s 174ms/step - loss: 0.0741 - accuracy: 0.9877\n",
      "34\n",
      "10/10 [==============================] - 2s 180ms/step - loss: 0.0490 - accuracy: 0.9923\n",
      "35\n",
      "10/10 [==============================] - 2s 177ms/step - loss: 0.0562 - accuracy: 0.9900\n",
      "36\n",
      "10/10 [==============================] - 2s 189ms/step - loss: 0.0385 - accuracy: 0.9926\n",
      "37\n",
      "10/10 [==============================] - 2s 182ms/step - loss: 0.0331 - accuracy: 0.9956\n",
      "38\n",
      "10/10 [==============================] - 2s 187ms/step - loss: 0.0521 - accuracy: 0.9911\n",
      "39\n",
      "10/10 [==============================] - 2s 187ms/step - loss: 0.0386 - accuracy: 0.9921\n",
      "40\n",
      "10/10 [==============================] - 2s 191ms/step - loss: 0.0309 - accuracy: 0.9950\n",
      "41\n",
      "10/10 [==============================] - 2s 195ms/step - loss: 0.0361 - accuracy: 0.9950\n",
      "42\n",
      "10/10 [==============================] - 2s 211ms/step - loss: 0.0243 - accuracy: 0.9966\n",
      "43\n",
      "10/10 [==============================] - 2s 201ms/step - loss: 0.0241 - accuracy: 0.9966\n",
      "44\n",
      "10/10 [==============================] - 2s 198ms/step - loss: 0.0270 - accuracy: 0.9950\n",
      "45\n",
      "10/10 [==============================] - 2s 224ms/step - loss: 0.0281 - accuracy: 0.9950\n",
      "46\n",
      "10/10 [==============================] - 2s 194ms/step - loss: 0.0152 - accuracy: 0.9980\n",
      "47\n",
      "10/10 [==============================] - 2s 213ms/step - loss: 0.0251 - accuracy: 0.9965\n",
      "48\n",
      "10/10 [==============================] - 2s 205ms/step - loss: 0.0153 - accuracy: 0.9974\n",
      "49\n",
      "10/10 [==============================] - 2s 205ms/step - loss: 0.0196 - accuracy: 0.9971\n",
      "50\n",
      "10/10 [==============================] - 2s 221ms/step - loss: 0.0254 - accuracy: 0.9953\n",
      "51\n",
      "10/10 [==============================] - 2s 222ms/step - loss: 0.0125 - accuracy: 0.9984\n",
      "52\n",
      "10/10 [==============================] - 2s 216ms/step - loss: 0.0246 - accuracy: 0.9953\n",
      "53\n",
      "10/10 [==============================] - 2s 226ms/step - loss: 0.0146 - accuracy: 0.9973\n",
      "54\n",
      "10/10 [==============================] - 3s 253ms/step - loss: 0.0086 - accuracy: 0.9985\n",
      "55\n",
      "10/10 [==============================] - 2s 224ms/step - loss: 0.0083 - accuracy: 0.9991\n",
      "56\n",
      "10/10 [==============================] - 3s 252ms/step - loss: 0.0123 - accuracy: 0.9979\n",
      "57\n",
      "10/10 [==============================] - 2s 235ms/step - loss: 0.0276 - accuracy: 0.9941\n",
      "58\n",
      "10/10 [==============================] - 2s 231ms/step - loss: 0.0137 - accuracy: 0.9974\n",
      "59\n",
      "10/10 [==============================] - 2s 225ms/step - loss: 0.0136 - accuracy: 0.9971\n",
      "60\n",
      "10/10 [==============================] - 2s 237ms/step - loss: 0.0129 - accuracy: 0.9971\n",
      "61\n",
      "10/10 [==============================] - 2s 248ms/step - loss: 0.0125 - accuracy: 0.9980\n",
      "62\n",
      "10/10 [==============================] - 3s 253ms/step - loss: 0.0117 - accuracy: 0.9975\n",
      "63\n",
      "10/10 [==============================] - 3s 266ms/step - loss: 0.0241 - accuracy: 0.9958\n",
      "64\n",
      "10/10 [==============================] - 2s 240ms/step - loss: 0.0153 - accuracy: 0.9978\n",
      "65\n",
      "10/10 [==============================] - 3s 266ms/step - loss: 0.0077 - accuracy: 0.9992\n",
      "66\n",
      "10/10 [==============================] - 2s 245ms/step - loss: 0.0125 - accuracy: 0.9984\n",
      "67\n",
      "10/10 [==============================] - 2s 244ms/step - loss: 0.0067 - accuracy: 0.9989\n",
      "68\n",
      "10/10 [==============================] - 3s 254ms/step - loss: 0.0101 - accuracy: 0.9982\n",
      "69\n",
      "10/10 [==============================] - 3s 262ms/step - loss: 0.0097 - accuracy: 0.9982\n",
      "70\n",
      "10/10 [==============================] - 3s 261ms/step - loss: 0.0044 - accuracy: 0.9997\n",
      "71\n",
      "10/10 [==============================] - 3s 263ms/step - loss: 0.0053 - accuracy: 0.9995\n",
      "72\n",
      "10/10 [==============================] - 3s 258ms/step - loss: 0.0054 - accuracy: 0.9990\n",
      "73\n",
      "10/10 [==============================] - 3s 253ms/step - loss: 0.0136 - accuracy: 0.9975\n",
      "74\n",
      "10/10 [==============================] - 3s 266ms/step - loss: 0.0096 - accuracy: 0.9980\n",
      "75\n",
      "10/10 [==============================] - 3s 279ms/step - loss: 0.0037 - accuracy: 0.9995\n",
      "76\n",
      "10/10 [==============================] - 3s 260ms/step - loss: 0.0055 - accuracy: 0.9990\n",
      "77\n",
      "10/10 [==============================] - 3s 288ms/step - loss: 0.0032 - accuracy: 0.9998\n",
      "78\n",
      "10/10 [==============================] - 3s 286ms/step - loss: 0.0050 - accuracy: 0.9993\n",
      "79\n",
      "10/10 [==============================] - 3s 275ms/step - loss: 0.0057 - accuracy: 0.9991\n",
      "80\n",
      "10/10 [==============================] - 3s 275ms/step - loss: 0.0072 - accuracy: 0.9986\n",
      "81\n",
      "10/10 [==============================] - 3s 292ms/step - loss: 0.0091 - accuracy: 0.9988\n",
      "82\n",
      "10/10 [==============================] - 3s 313ms/step - loss: 0.0059 - accuracy: 0.9993\n",
      "83\n",
      "10/10 [==============================] - 3s 293ms/step - loss: 0.0043 - accuracy: 0.9995\n",
      "84\n",
      "10/10 [==============================] - 3s 286ms/step - loss: 0.0023 - accuracy: 0.9996\n",
      "85\n",
      "10/10 [==============================] - 3s 294ms/step - loss: 0.0081 - accuracy: 0.9982\n",
      "86\n",
      "10/10 [==============================] - 3s 307ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "87\n",
      "10/10 [==============================] - 3s 295ms/step - loss: 0.0024 - accuracy: 0.9998\n",
      "88\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 3s 315ms/step - loss: 0.0041 - accuracy: 0.9996\n",
      "89\n",
      "10/10 [==============================] - 3s 301ms/step - loss: 0.0032 - accuracy: 0.9996\n",
      "90\n",
      "10/10 [==============================] - 3s 298ms/step - loss: 0.0059 - accuracy: 0.9987\n",
      "91\n",
      "10/10 [==============================] - 3s 328ms/step - loss: 0.0024 - accuracy: 0.9996\n",
      "92\n",
      "10/10 [==============================] - 3s 308ms/step - loss: 0.0058 - accuracy: 0.9992\n",
      "93\n",
      "10/10 [==============================] - 3s 325ms/step - loss: 0.0019 - accuracy: 0.9996\n",
      "94\n",
      "10/10 [==============================] - 3s 321ms/step - loss: 0.0051 - accuracy: 0.9992\n",
      "95\n",
      "10/10 [==============================] - 3s 318ms/step - loss: 0.0063 - accuracy: 0.9994\n",
      "96\n",
      "10/10 [==============================] - 3s 319ms/step - loss: 0.0031 - accuracy: 0.9998\n",
      "97\n",
      "10/10 [==============================] - 3s 329ms/step - loss: 0.0027 - accuracy: 0.9998\n",
      "98\n",
      "10/10 [==============================] - 3s 350ms/step - loss: 0.0036 - accuracy: 0.9994\n",
      "99\n",
      "10/10 [==============================] - 4s 418ms/step - loss: 0.0021 - accuracy: 0.9998\n",
      "100\n",
      "10/10 [==============================] - 4s 401ms/step - loss: 0.0037 - accuracy: 0.9992\n",
      "101\n",
      "10/10 [==============================] - 3s 338ms/step - loss: 3.6480e-04 - accuracy: 1.0000\n",
      "102\n",
      "10/10 [==============================] - 4s 354ms/step - loss: 0.0023 - accuracy: 0.9996\n",
      "103\n",
      "10/10 [==============================] - 4s 377ms/step - loss: 0.0024 - accuracy: 0.9996\n",
      "104\n",
      "10/10 [==============================] - 4s 362ms/step - loss: 0.0020 - accuracy: 0.9998\n",
      "105\n",
      "10/10 [==============================] - 4s 357ms/step - loss: 0.0010 - accuracy: 0.9998\n",
      "106\n",
      "10/10 [==============================] - 4s 355ms/step - loss: 0.0028 - accuracy: 0.9996\n",
      "107\n",
      "10/10 [==============================] - 4s 371ms/step - loss: 0.0014 - accuracy: 0.9998\n",
      "108\n",
      "10/10 [==============================] - 4s 368ms/step - loss: 0.0015 - accuracy: 0.9995\n",
      "109\n",
      "10/10 [==============================] - 4s 355ms/step - loss: 0.0013 - accuracy: 0.9998\n",
      "110\n",
      "10/10 [==============================] - 4s 359ms/step - loss: 4.7419e-04 - accuracy: 1.0000\n",
      "111\n",
      "10/10 [==============================] - 4s 371ms/step - loss: 0.0016 - accuracy: 0.9997\n",
      "112\n",
      "10/10 [==============================] - 4s 376ms/step - loss: 0.0018 - accuracy: 0.9995\n",
      "113\n",
      "10/10 [==============================] - 4s 380ms/step - loss: 1.8311e-04 - accuracy: 1.0000\n",
      "114\n",
      "10/10 [==============================] - 4s 393ms/step - loss: 0.0010 - accuracy: 0.9998\n",
      "115\n",
      "10/10 [==============================] - 4s 397ms/step - loss: 4.1002e-04 - accuracy: 1.0000\n",
      "116\n",
      "10/10 [==============================] - 4s 388ms/step - loss: 0.0013 - accuracy: 0.9997\n",
      "117\n",
      "10/10 [==============================] - 4s 394ms/step - loss: 0.0024 - accuracy: 0.9995\n",
      "118\n",
      "10/10 [==============================] - 4s 391ms/step - loss: 2.9512e-04 - accuracy: 1.0000\n",
      "119\n",
      "10/10 [==============================] - 4s 405ms/step - loss: 3.5958e-04 - accuracy: 1.0000\n",
      "120\n",
      "10/10 [==============================] - 4s 384ms/step - loss: 0.0029 - accuracy: 0.9994\n",
      "121\n",
      "10/10 [==============================] - 4s 411ms/step - loss: 0.0016 - accuracy: 0.9997\n",
      "122\n",
      "10/10 [==============================] - 4s 400ms/step - loss: 0.0015 - accuracy: 0.9997\n",
      "123\n",
      "10/10 [==============================] - 4s 424ms/step - loss: 8.9449e-04 - accuracy: 1.0000\n",
      "124\n",
      "10/10 [==============================] - 4s 407ms/step - loss: 0.0025 - accuracy: 0.9997\n",
      "125\n",
      "10/10 [==============================] - 4s 431ms/step - loss: 2.6791e-04 - accuracy: 1.0000\n",
      "126\n",
      "10/10 [==============================] - 4s 410ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "127\n",
      "10/10 [==============================] - 4s 426ms/step - loss: 0.0011 - accuracy: 0.9997\n",
      "128\n",
      "10/10 [==============================] - 5s 453ms/step - loss: 2.3118e-04 - accuracy: 1.0000\n",
      "129\n",
      "10/10 [==============================] - 4s 423ms/step - loss: 0.0010 - accuracy: 1.0000\n",
      "130\n",
      "10/10 [==============================] - 5s 461ms/step - loss: 2.9646e-04 - accuracy: 1.0000\n",
      "131\n",
      "10/10 [==============================] - 5s 457ms/step - loss: 0.0019 - accuracy: 0.9997\n",
      "132\n",
      "10/10 [==============================] - 4s 442ms/step - loss: 2.8194e-04 - accuracy: 1.0000\n",
      "133\n",
      "10/10 [==============================] - 4s 445ms/step - loss: 6.5880e-04 - accuracy: 0.9999\n",
      "134\n",
      "10/10 [==============================] - 5s 474ms/step - loss: 2.6468e-04 - accuracy: 1.0000\n",
      "135\n",
      "10/10 [==============================] - 5s 453ms/step - loss: 5.2065e-04 - accuracy: 1.0000\n",
      "136\n",
      "10/10 [==============================] - 5s 475ms/step - loss: 5.0589e-04 - accuracy: 0.9999\n",
      "137\n",
      "10/10 [==============================] - 5s 461ms/step - loss: 6.5275e-04 - accuracy: 0.9999\n",
      "138\n",
      "10/10 [==============================] - 5s 464ms/step - loss: 0.0012 - accuracy: 0.9999\n",
      "139\n",
      "10/10 [==============================] - 5s 490ms/step - loss: 5.8546e-04 - accuracy: 1.0000\n",
      "140\n",
      "10/10 [==============================] - 5s 484ms/step - loss: 2.3066e-04 - accuracy: 1.0000\n",
      "141\n",
      "10/10 [==============================] - 5s 492ms/step - loss: 1.8368e-04 - accuracy: 1.0000\n",
      "142\n",
      "10/10 [==============================] - 5s 496ms/step - loss: 3.1433e-04 - accuracy: 1.0000\n",
      "143\n",
      "10/10 [==============================] - 5s 532ms/step - loss: 1.4497e-04 - accuracy: 1.0000\n",
      "144\n",
      "10/10 [==============================] - 5s 496ms/step - loss: 1.0246e-04 - accuracy: 1.0000\n",
      "145\n",
      "10/10 [==============================] - 5s 508ms/step - loss: 1.4553e-04 - accuracy: 1.0000\n",
      "146\n",
      "10/10 [==============================] - 5s 538ms/step - loss: 9.2601e-04 - accuracy: 0.9999\n",
      "147\n",
      "10/10 [==============================] - 5s 532ms/step - loss: 1.2200e-04 - accuracy: 1.0000\n",
      "148\n",
      "10/10 [==============================] - 5s 533ms/step - loss: 1.5392e-04 - accuracy: 1.0000\n",
      "149\n",
      "10/10 [==============================] - 5s 519ms/step - loss: 2.2316e-04 - accuracy: 1.0000\n",
      "150\n",
      "10/10 [==============================] - 5s 537ms/step - loss: 0.0011 - accuracy: 0.9998\n",
      "151\n",
      "10/10 [==============================] - 5s 525ms/step - loss: 4.2909e-04 - accuracy: 1.0000\n",
      "152\n",
      "10/10 [==============================] - 6s 556ms/step - loss: 5.0608e-05 - accuracy: 1.0000\n",
      "153\n",
      "10/10 [==============================] - 5s 520ms/step - loss: 6.3190e-05 - accuracy: 1.0000\n",
      "154\n",
      "10/10 [==============================] - 6s 628ms/step - loss: 2.0729e-04 - accuracy: 0.9999\n",
      "155\n",
      "10/10 [==============================] - 6s 559ms/step - loss: 1.9534e-04 - accuracy: 1.0000\n",
      "156\n",
      "10/10 [==============================] - 6s 598ms/step - loss: 3.1787e-04 - accuracy: 1.0000\n",
      "157\n",
      "10/10 [==============================] - 6s 575ms/step - loss: 1.6102e-04 - accuracy: 1.0000\n",
      "158\n",
      "10/10 [==============================] - 6s 589ms/step - loss: 1.8166e-04 - accuracy: 1.0000\n",
      "159\n",
      "10/10 [==============================] - 6s 604ms/step - loss: 2.0963e-05 - accuracy: 1.0000\n",
      "160\n",
      "10/10 [==============================] - 6s 571ms/step - loss: 2.4736e-05 - accuracy: 1.0000\n",
      "161\n",
      "10/10 [==============================] - 6s 627ms/step - loss: 1.2779e-05 - accuracy: 1.0000\n",
      "162\n",
      "10/10 [==============================] - 7s 701ms/step - loss: 9.2215e-04 - accuracy: 0.9999\n",
      "163\n",
      "10/10 [==============================] - 7s 695ms/step - loss: 2.2845e-04 - accuracy: 1.0000\n",
      "164\n",
      "10/10 [==============================] - 6s 619ms/step - loss: 3.6139e-05 - accuracy: 1.0000\n",
      "165\n",
      "10/10 [==============================] - 6s 618ms/step - loss: 2.1787e-05 - accuracy: 1.0000\n",
      "166\n",
      "10/10 [==============================] - 6s 626ms/step - loss: 1.2914e-05 - accuracy: 1.0000\n",
      "167\n",
      "10/10 [==============================] - 6s 605ms/step - loss: 2.5934e-05 - accuracy: 1.0000\n",
      "168\n",
      "StopIteration\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "while True:\n",
    "    try:\n",
    "        noised_tweets_batch, original_tweets_batch, original_tweets_delayed_batch = next(batches_gen)\n",
    "        model.fit([noised_tweets_batch, original_tweets_delayed_batch], original_tweets_batch,\n",
    "                  steps_per_epoch=10, epochs=1)\n",
    "    except StopIteration:\n",
    "        print('StopIteration')\n",
    "        break\n",
    "    print(i)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T12:02:08.975170Z",
     "start_time": "2021-06-03T12:02:08.948742Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save(\"one_pass.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T14:16:31.237753Z",
     "start_time": "2021-06-03T14:16:30.815016Z"
    }
   },
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"one_pass.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T14:16:32.603589Z",
     "start_time": "2021-06-03T14:16:32.422353Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder_inputs = model.input[0]  # input_1\n",
    "encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output  # lstm_1\n",
    "encoder_states = [state_h_enc, state_c_enc]\n",
    "encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_inputs = model.input[1]  # input_2\n",
    "decoder_state_input_h = keras.Input(shape=(latent_dim,), name=\"decoder_state_input_h\")\n",
    "decoder_state_input_c = keras.Input(shape=(latent_dim,), name=\"decoder_state_input_c\")\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_lstm = model.layers[3]\n",
    "decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs\n",
    ")\n",
    "decoder_states = [state_h_dec, state_c_dec]\n",
    "decoder_dense = model.layers[4]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = keras.Model(\n",
    "    [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T14:16:33.202145Z",
     "start_time": "2021-06-03T14:16:33.199316Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict((i, char) for char, i in VOCAB_TO_INT.items())\n",
    "reverse_target_char_index = reverse_input_char_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T14:16:34.021674Z",
     "start_time": "2021-06-03T14:16:34.015508Z"
    }
   },
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    # print('states values:', states_value)\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, NUM_TOKENS))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, VOCAB_TO_INT['<GO>']] = 1.0\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if sampled_char == \"\\n\" or len(decoded_sentence) > 100:\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, NUM_TOKENS))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.0\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T14:16:47.176840Z",
     "start_time": "2021-06-03T14:16:34.895146Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target clean tweet: cuando no ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/d/opt/anaconda3/envs/py_3_7_tf_2_5/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py:3704: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable.debug_mode()`.\n",
      "  \"Even though the `tf.config.experimental_run_functions_eagerly` \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded tweet: ¬°¬°¬°¬°¬°¬°¬°¬°<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "target clean tweet: graciass :)\n",
      "Decoded tweet: ¬°¬°¬°¬°¬°¬°¬°¬°<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "target clean tweet: jajajajajja\n",
      "Decoded tweet: ¬°¬°¬°¬°¬°¬°¬°¬°<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n"
     ]
    }
   ],
   "source": [
    "batches_gen = get_batches(clean_sorted_medium_lines, noiser, batch_size=1)\n",
    "for i in range(3):\n",
    "    noised_tweets_batch, original_tweets_batch, original_tweets_delayed_batch = next(batches_gen)\n",
    "    print(\"target clean tweet:\", clean_sorted_medium_lines[i])\n",
    "    decoded_tweet = decode_sequence(noised_tweets_batch)\n",
    "    print(\"Decoded tweet:\", decoded_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
